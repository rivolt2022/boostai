import pandas as pd
import numpy as np
import os
import random
import pickle
from rdkit import Chem
from rdkit.Chem import AllChem, DataStructs, Descriptors, rdMolDescriptors
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.model_selection import KFold, StratifiedShuffleSplit
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import lightgbm as lgb
import optuna
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from gensim.models.word2vec import Word2Vec

warnings.filterwarnings('ignore')

# ì „ì—­ ì„¤ì • ë³€ìˆ˜ë“¤
CFG = {
    'NBITS': 2048,      # Morgan ì§€ë¬¸ì˜ ë¹„íŠ¸ ìˆ˜
    'SEED': 42,         # ì¬í˜„ì„±ì„ ìœ„í•œ ëœë¤ ì‹œë“œ
    'N_SPLITS': 10,     # K-í´ë“œ êµì°¨ ê²€ì¦ì—ì„œ ì‚¬ìš©í•  í´ë“œ ìˆ˜
    'N_TRIALS': 100,    # Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œë„ íšŸìˆ˜ (ì¦ê°€)
    'USE_WORD2VEC': True,  # Word2Vec ëª¨ë¸ ì‚¬ìš© ì—¬ë¶€
    'WORD2VEC_DIM': 300,   # Word2Vec ë²¡í„° ì°¨ì›
    'USE_ENSEMBLE': True,  # ì•™ìƒë¸” ëª¨ë¸ ì‚¬ìš©
    'ENSEMBLE_MODELS': ['lgb']  # LightGBMë§Œ ì‚¬ìš©
}

def seed_everything(seed):
    """ëª¨ë“  ëœë¤ ì‹œë“œë¥¼ ì„¤ì •í•˜ì—¬ ì‹¤í—˜ì˜ ì¬í˜„ì„±ì„ ë³´ì¥í•˜ëŠ” í•¨ìˆ˜"""
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)

# ì „ì—­ ì‹œë“œ ì„¤ì •
seed_everything(CFG['SEED'])

class CYP3A4InhibitionPredictor:
    def __init__(self):
        self.models = {}
        self.scaler = RobustScaler()
        self.feature_names = None
        self.best_params = {}
        self.word2vec_model = None
        self.descriptor_names = [desc_name for desc_name, _ in Descriptors._descList]
        
        # Word2Vec ëª¨ë¸ ë¡œë“œ
        if CFG['USE_WORD2VEC']:
            try:
                with open('model_300dim.pkl', 'rb') as f:
                    self.word2vec_model = pickle.load(f)
                print("âœ… Word2Vec ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
            except Exception as e:
                print(f"âŒ Word2Vec ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                print("Word2Vec ì—†ì´ ì§„í–‰í•©ë‹ˆë‹¤.")
                CFG['USE_WORD2VEC'] = False

    def tokenize_smiles(self, smiles):
        """SMILESë¥¼ í† í°ìœ¼ë¡œ ë¶„ë¦¬í•˜ëŠ” í•¨ìˆ˜"""
        tokens = []
        i = 0
        while i < len(smiles):
            # ë‘ ê¸€ì í† í°ë“¤ ë¨¼ì € í™•ì¸
            if i < len(smiles) - 1:
                two_char = smiles[i:i+2]
                if two_char in ['Cl', 'Br', 'Si', 'Na', 'Mg', 'Al', 'Ca', 'Fe', 'Cu', 'Zn']:
                    tokens.append(two_char)
                    i += 2
                    continue
            
            # í•œ ê¸€ì í† í°
            tokens.append(smiles[i])
            i += 1
        
        return tokens

    def smiles_to_word2vec(self, smiles):
        """SMILESë¥¼ Word2Vec ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜"""
        if not CFG['USE_WORD2VEC'] or self.word2vec_model is None:
            return np.zeros(CFG['WORD2VEC_DIM'])
        
        try:
            tokens = self.tokenize_smiles(smiles)
            vectors = []
            
            for token in tokens:
                try:
                    # ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ ë²¡í„° ê°€ì ¸ì˜¤ê¸° ì‹œë„
                    vec = None
                    if hasattr(self.word2vec_model.wv, 'word_vec'):
                        vec = self.word2vec_model.wv.word_vec(token)
                    elif hasattr(self.word2vec_model.wv, 'get_vector'):
                        vec = self.word2vec_model.wv.get_vector(token)
                    elif hasattr(self.word2vec_model.wv, '__getitem__'):
                        vec = self.word2vec_model.wv[token]
                    else:
                        # vocabì—ì„œ ì§ì ‘ ì ‘ê·¼ ì‹œë„
                        if hasattr(self.word2vec_model.wv, 'vocab') and token in self.word2vec_model.wv.vocab:
                            idx = self.word2vec_model.wv.vocab[token].index
                            vec = self.word2vec_model.wv.syn0[idx]
                        elif hasattr(self.word2vec_model.wv, 'index2word'):
                            # index2wordë¥¼ ì‚¬ìš©í•´ì„œ ì°¾ê¸°
                            try:
                                idx = self.word2vec_model.wv.index2word.index(token)
                                vec = self.word2vec_model.wv.syn0[idx]
                            except ValueError:
                                continue
                    
                    if vec is not None:
                        vectors.append(vec)
                        
                except (KeyError, ValueError, AttributeError):
                    # í† í°ì´ ì–´íœ˜ì— ì—†ëŠ” ê²½ìš° ê±´ë„ˆë›°ê¸°
                    continue
            
            if vectors:
                # ëª¨ë“  í† í° ë²¡í„°ì˜ í‰ê·  ê³„ì‚°
                return np.mean(vectors, axis=0)
            else:
                return np.zeros(CFG['WORD2VEC_DIM'])
                
        except Exception as e:
            print(f"Word2Vec ë³€í™˜ ì˜¤ë¥˜ ({smiles}): {e}")
            return np.zeros(CFG['WORD2VEC_DIM'])

    def get_all_descriptors(self, mol):
        """RDKitì˜ ëª¨ë“  ë¶„ì ì„¤ëª…ìë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜"""
        desc_dict = {}
        for name in self.descriptor_names:
            try:
                desc_func = getattr(Descriptors, name)
                desc_dict[name] = desc_func(mol)
            except:
                desc_dict[name] = 0
        return desc_dict

    def smiles_to_features(self, smiles):
        """SMILES ë¬¸ìì—´ì—ì„œ ëª¨ë“  íŠ¹ì„±ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜"""
        mol = Chem.MolFromSmiles(smiles)
        if mol is None:
            return None, None, None, None

        # 1. Morgan Fingerprint
        fp_morgan = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=CFG['NBITS'])
        arr_morgan = np.zeros((1,))
        DataStructs.ConvertToNumpyArray(fp_morgan, arr_morgan)

        # 2. MACCS Keys
        fp_maccs = AllChem.GetMACCSKeysFingerprint(mol)
        arr_maccs = np.zeros((1,))
        DataStructs.ConvertToNumpyArray(fp_maccs, arr_maccs)

        # 3. RDKit Descriptors
        descriptors = self.get_all_descriptors(mol)

        # 4. Word2Vec ë²¡í„°
        word2vec_vec = self.smiles_to_word2vec(smiles)

        return arr_morgan, arr_maccs, descriptors, word2vec_vec

    def prepare_data(self, df):
        """ë°ì´í„°í”„ë ˆì„ì—ì„œ ëª¨ë“  íŠ¹ì„±ì„ ë³‘ë ¬ë¡œ ì¶”ì¶œ"""
        print("ë¶„ì íŠ¹ì„± ì¶”ì¶œ ì¤‘ (Morgan, MACCS, RDKit Descriptors, Word2Vec)...")
        
        all_features = []
        for i, smiles in enumerate(df['Canonical_Smiles']):
            if i % 200 == 0:
                print(f"ì²˜ë¦¬ ì¤‘: {i}/{len(df)}")
            
            morgan_fp, maccs_fp, descriptors, word2vec_vec = self.smiles_to_features(smiles)
            
            if morgan_fp is None:
                # SMILES íŒŒì‹± ì‹¤íŒ¨ ì‹œ
                morgan_fp = np.zeros(CFG['NBITS'])
                maccs_fp = np.zeros(167)
                descriptors = {name: 0 for name in self.descriptor_names}
                word2vec_vec = np.zeros(CFG['WORD2VEC_DIM'])

            all_features.append((morgan_fp, maccs_fp, list(descriptors.values()), word2vec_vec))

        # íŠ¹ì„±ë³„ë¡œ ë¶„ë¦¬
        morgan_fps = np.array([item[0] for item in all_features])
        maccs_fps = np.array([item[1] for item in all_features])
        desc_df = pd.DataFrame([item[2] for item in all_features], columns=self.descriptor_names)
        word2vec_vecs = np.array([item[3] for item in all_features])

        # íŠ¹ì„± ì´ë¦„ ì €ì¥
        morgan_names = [f'Morgan_{i}' for i in range(CFG['NBITS'])]
        maccs_names = [f'MACCS_{i}' for i in range(maccs_fps.shape[1])]
        word2vec_names = [f'Word2Vec_{i}' for i in range(CFG['WORD2VEC_DIM'])]
        
        self.feature_names = morgan_names + maccs_names + self.descriptor_names + word2vec_names
        
        # ëª¨ë“  íŠ¹ì„±ì„ í•˜ë‚˜ì˜ numpy ë°°ì—´ë¡œ ê²°í•©
        return np.hstack([morgan_fps, maccs_fps, desc_df.values, word2vec_vecs])

    def get_score(self, y_true, y_pred):
        """ë¦¬ë”ë³´ë“œ í‰ê°€ ì§€í‘œì— ë§ëŠ” ìŠ¤ì½”ì–´ í•¨ìˆ˜
        Score = 0.5 * (1 - min(A, 1)) + 0.5 * B
        A = Normalized RMSE = RMSE / (max(y) - min(y))
        B = Pearson Correlation Coefficient (clipped to [0, 1])
        """
        # A: Normalized RMSE
        mse = mean_squared_error(y_true, y_pred)
        rmse = np.sqrt(mse)
        y_range = np.max(y_true) - np.min(y_true)
        normalized_rmse = rmse / y_range
        A = min(normalized_rmse, 1)  # 1ë¡œ í´ë¦¬í•‘
        
        # B: Pearson Correlation Coefficient (clipped to [0, 1])
        correlation = np.corrcoef(y_true, y_pred)[0, 1]
        if np.isnan(correlation):
            correlation = 0
        B = np.clip(correlation, 0, 1)
        
        # ìµœì¢… ìŠ¤ì½”ì–´ ê³„ì‚°
        score = 0.5 * (1 - A) + 0.5 * B
        
        return score

    def objective_lgb(self, trial, X, y):
        """LightGBM Optuna ëª©ì  í•¨ìˆ˜"""
        params = {
            'objective': 'regression',
            'metric': 'rmse',
            'verbose': -1,
            'n_jobs': -1,
            'seed': CFG['SEED'],
            'boosting_type': 'gbdt',
            'n_estimators': trial.suggest_int('n_estimators', 500, 2000),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),
            'num_leaves': trial.suggest_int('num_leaves', 20, 100),
            'max_depth': trial.suggest_int('max_depth', 5, 15),
            'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 0.9),
            'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 0.9),
            'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),
            'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),
            'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),
            'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),
        }

        kf = KFold(n_splits=5, shuffle=True, random_state=CFG['SEED'])
        oof_preds = np.zeros(len(X))
        y_array = y.values if hasattr(y, 'values') else np.array(y)

        for train_idx, val_idx in kf.split(X, y_array):
            X_train, X_val = X[train_idx], X[val_idx]
            y_train, y_val = y_array[train_idx], y_array[val_idx]

            scaler = RobustScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_val_scaled = scaler.transform(X_val)

            model = lgb.LGBMRegressor(**params)
            model.fit(X_train_scaled, y_train, eval_set=[(X_val_scaled, y_val)],
                      eval_metric='rmse', callbacks=[lgb.early_stopping(100, verbose=False)])
            
            oof_preds[val_idx] = model.predict(X_val_scaled)

        score = self.get_score(y_array, oof_preds)
        return score

    def train_and_predict(self, X_train_full, y_train_full, X_test_full):
        """ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡"""
        print("ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
        
        # ê° ëª¨ë¸ë³„ ìµœì í™”
        for model_name in CFG['ENSEMBLE_MODELS']:
            print(f"\n{model_name.upper()} ëª¨ë¸ ìµœì í™” ì¤‘...")
            
            if model_name == 'lgb':
                study = optuna.create_study(direction='maximize', study_name=f'{model_name}_tuning')
                study.optimize(lambda trial: self.objective_lgb(trial, X_train_full, y_train_full), 
                              n_trials=CFG['N_TRIALS']//3)
                self.best_params[model_name] = study.best_params
                print(f"{model_name} ìµœê³  ìŠ¤ì½”ì–´: {study.best_value:.4f}")
                
        # K-Fold ì•™ìƒë¸” í›ˆë ¨ ë° ì˜ˆì¸¡
        print("\nì•™ìƒë¸” ì˜ˆì¸¡ì„ ìœ„í•œ K-í´ë“œ í›ˆë ¨ ì¤‘...")
        kf = KFold(n_splits=CFG['N_SPLITS'], shuffle=True, random_state=CFG['SEED'])
        test_preds_ensemble = np.zeros(len(X_test_full))
        oof_preds_ensemble = np.zeros(len(X_train_full))
        
        # ê° ëª¨ë¸ë³„ ì˜ˆì¸¡ ì €ì¥
        test_preds_models = {model: np.zeros(len(X_test_full)) for model in CFG['ENSEMBLE_MODELS']}
        oof_preds_models = {model: np.zeros(len(X_train_full)) for model in CFG['ENSEMBLE_MODELS']}

        for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_full, y_train_full)):
            print(f"--- í›ˆë ¨ í´ë“œ {fold+1}/{CFG['N_SPLITS']} ---")
            X_train, X_val = X_train_full[train_idx], X_train_full[val_idx]
            y_train, y_val = y_train_full.iloc[train_idx], y_train_full.iloc[val_idx]

            scaler = RobustScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_val_scaled = scaler.transform(X_val)
            X_test_scaled = scaler.transform(X_test_full)

            # ê° ëª¨ë¸ í›ˆë ¨
            fold_models = {}
            
            # LightGBM
            if 'lgb' in CFG['ENSEMBLE_MODELS']:
                lgb_params = {
                    'objective': 'regression', 'metric': 'rmse', 'verbose': -1, 'n_jobs': -1,
                    'seed': CFG['SEED'], 'boosting_type': 'gbdt'
                }
                lgb_params.update(self.best_params['lgb'])
                lgb_model = lgb.LGBMRegressor(**lgb_params)
                lgb_model.fit(X_train_scaled, y_train, eval_set=[(X_val_scaled, y_val)],
                             eval_metric='rmse', callbacks=[lgb.early_stopping(100, verbose=False)])
                fold_models['lgb'] = lgb_model
                
                oof_preds_models['lgb'][val_idx] = lgb_model.predict(X_val_scaled)
                test_preds_models['lgb'] += lgb_model.predict(X_test_scaled) / CFG['N_SPLITS']

            # ì•™ìƒë¸” ì˜ˆì¸¡ (ê°€ì¤‘ í‰ê· )
            weights = {'lgb': 1.0}  # LightGBMë§Œ ì‚¬ìš©
            
            for model_name in CFG['ENSEMBLE_MODELS']:
                if model_name in fold_models:
                    oof_preds_ensemble[val_idx] += weights[model_name] * oof_preds_models[model_name][val_idx]
                    test_preds_ensemble += weights[model_name] * test_preds_models[model_name] / CFG['N_SPLITS']

        # ìµœì¢… ì„±ëŠ¥ í‰ê°€
        final_score = self.get_score(y_train_full, oof_preds_ensemble)
        print(f"\nì•™ìƒë¸” ëª¨ë¸ ìµœì¢… ìŠ¤ì½”ì–´: {final_score:.4f}")
        
        # ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ë„ ì¶œë ¥
        for model_name in CFG['ENSEMBLE_MODELS']:
            if model_name in oof_preds_models:
                model_score = self.get_score(y_train_full, oof_preds_models[model_name])
                print(f"{model_name.upper()} ê°œë³„ ìŠ¤ì½”ì–´: {model_score:.4f}")
        
        self.plot_results(y_train_full, oof_preds_ensemble, "ì•™ìƒë¸” ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼")

        return test_preds_ensemble

    def plot_results(self, y_true, y_pred, title="ì˜ˆì¸¡ vs ì‹¤ì œ"):
        """ê²°ê³¼ ì‹œê°í™”"""
        plt.figure(figsize=(14, 8))
        plt.rcParams['font.family'] = 'Malgun Gothic'
        plt.rcParams['axes.unicode_minus'] = False
        
        # ì‚°ì ë„
        plt.subplot(2, 2, 1)
        sns.regplot(x=y_true, y=y_pred, scatter_kws={'alpha':0.4, 'color': 'royalblue'}, line_kws={'color':'red', 'linestyle':'--'})
        plt.xlabel('ì‹¤ì œ ê°’ (Inhibition %)')
        plt.ylabel('ì˜ˆì¸¡ ê°’ (Inhibition %)')
        plt.title(f'{title}\nRÂ² = {r2_score(y_true, y_pred):.4f}')
        
        # ì”ì°¨ í”Œë¡¯
        plt.subplot(2, 2, 2)
        residuals = y_true - y_pred
        sns.scatterplot(x=y_pred, y=residuals, alpha=0.5, color='forestgreen')
        plt.axhline(y=0, color='r', linestyle='--')
        plt.xlabel('ì˜ˆì¸¡ ê°’')
        plt.ylabel('ì”ì°¨')
        plt.title('ì”ì°¨ í”Œë¡¯')
        
        # íŠ¹ì„± ì¤‘ìš”ë„ (LightGBM ê¸°ì¤€)
        plt.subplot(2, 1, 2)
        if self.feature_names and 'lgb' in self.models:
            importances = self.models['lgb'].feature_importances_
            feature_importance_df = pd.DataFrame({
                'feature': self.feature_names,
                'importance': importances
            }).sort_values('importance', ascending=False).head(20)
            
            sns.barplot(x='importance', y='feature', data=feature_importance_df)
            plt.title('ìƒìœ„ 20ê°œ íŠ¹ì„± ì¤‘ìš”ë„ (LightGBM ê¸°ì¤€)')
        
        plt.tight_layout()
        plt.savefig('model_results_ensemble.png', dpi=300, bbox_inches='tight')
        plt.show()

def main():
    print("ğŸš€ CYP3A4 íš¨ì†Œ ì €í•´ ì˜ˆì¸¡ ëª¨ë¸ (ì•™ìƒë¸” + Word2Vec) ì‹œì‘ ğŸš€")
    print("=" * 70)
    
    try:
        # ë°ì´í„° ë¡œë“œ
        print("ë°ì´í„° ë¡œë“œ ì¤‘...")
        train_df = pd.read_csv('data/train.csv')
        test_df = pd.read_csv('data/test.csv')
        sample_submission = pd.read_csv('data/sample_submission.csv')
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        predictor = CYP3A4InhibitionPredictor()
        
        # íŠ¹ì„± ì¶”ì¶œ
        X_train_full = predictor.prepare_data(train_df)
        X_test_full = predictor.prepare_data(test_df)
        y_train_full = train_df['Inhibition']
        
        print(f"\nìµœì¢… íŠ¹ì„± ìˆ˜: {X_train_full.shape[1]}")
        print(f"- Morgan Fingerprint: {CFG['NBITS']}")
        print(f"- MACCS Keys: 167")
        print(f"- RDKit Descriptors: {len(predictor.descriptor_names)}")
        print(f"- Word2Vec Vectors: {CFG['WORD2VEC_DIM']}")
        
        # ëª¨ë¸ í›ˆë ¨ ë° ì˜ˆì¸¡
        test_preds = predictor.train_and_predict(X_train_full, y_train_full, X_test_full)
        
        # ì œì¶œ íŒŒì¼ ìƒì„±
        submission = sample_submission.copy()
        submission['Inhibition'] = test_preds
        submission['Inhibition'] = np.clip(submission['Inhibition'], 0, 100)
        
        submission.to_csv('submission_ensemble.csv', index=False)
        print(f"\nâœ… ì œì¶œ íŒŒì¼ì´ 'submission_ensemble.csv'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        print("\nì˜ˆì¸¡ ê²°ê³¼ ìš”ì•½:")
        print(submission['Inhibition'].describe())

    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 