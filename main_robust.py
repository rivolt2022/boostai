# ğŸ¤– Word2Vec ê¸°ë°˜ CYP3A4 ì˜ˆì¸¡ ëª¨ë¸ (0.85+ ë„ì „)
import pandas as pd
import numpy as np
import os
import random
import pickle
from rdkit import Chem
from rdkit.Chem import AllChem, DataStructs, Descriptors
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import KFold
from sklearn.metrics import r2_score, mean_squared_error
import lightgbm as lgb
import optuna
import warnings

warnings.filterwarnings('ignore')

# ğŸ¤– Word2Vec ê¸°ë°˜ ì„¤ì • (0.85+ ëª©í‘œ)
CFG = {
    'NBITS': 2048,      # Morgan ì§€ë¬¸ ë¹„íŠ¸ ìˆ˜ (1024 ì‚¬ìš©)
    'SEEDS': [42, 123, 456, 789, 999],  # ğŸ›¡ï¸ ë‹¤ì¤‘ ì‹œë“œë¡œ ì•ˆì •ì„± í™•ë³´
    'N_SPLITS': 15,     # K-í´ë“œ ì¦ê°€ (ì•ˆì •ì„±)
    'N_TRIALS': 50,     # ë¹ ë¥¸ ì‹¤í–‰ì„ ìœ„í•´ ì¶•ì†Œ
    'SIMULATE_80_PERCENT': True,  # ğŸ² ë¬´ì‘ìœ„ 80% ì‹œë®¬ë ˆì´ì…˜
    'N_SIMULATIONS': 20,  # 80% ìƒ˜í”Œë§ ì‹œë®¬ë ˆì´ì…˜ íšŸìˆ˜
    'TARGET_TRANSFORM': True,  # ğŸ¯ íƒ€ê²Ÿ ë³€í™˜ í™œì„±í™”
    'WORD2VEC_DIM': 300,  # ğŸ¤– Word2Vec ë²¡í„° ì°¨ì›
}

def seed_everything(seed):
    """ëª¨ë“  ëœë¤ ì‹œë“œë¥¼ ì„¤ì •í•˜ì—¬ ì¬í˜„ì„± ë³´ì¥"""
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)

def simulate_random_80_percent(y_true, y_pred, n_simulations=20):
    """ğŸ² ë¬´ì‘ìœ„ 80% ìƒ˜í”Œë§ ì‹œë®¬ë ˆì´ì…˜ (ë¦¬ë”ë³´ë“œ í‰ê°€ ë°©ì‹ ëª¨ë°©)"""
    scores = []
    n_samples = len(y_true)
    
    for _ in range(n_simulations):
        # ë¬´ì‘ìœ„ 80% ìƒ˜í”Œë§
        indices = np.random.choice(n_samples, size=int(0.8 * n_samples), replace=False)
        y_true_sample = y_true[indices]
        y_pred_sample = y_pred[indices]
        
        # ìŠ¤ì½”ì–´ ê³„ì‚°
        rmse = np.sqrt(mean_squared_error(y_true_sample, y_pred_sample))
        y_range = y_true_sample.max() - y_true_sample.min()
        nrmse = rmse / y_range if y_range > 0 else 0
        
        correlation = np.corrcoef(y_true_sample, y_pred_sample)[0, 1]
        if np.isnan(correlation):
            correlation = 0
            
        score = 0.5 * (1 - nrmse) + 0.5 * correlation
        scores.append(score)
    
    return np.mean(scores), np.std(scores)

# ì²« ë²ˆì§¸ ì‹œë“œë¡œ ì´ˆê¸° ì„¤ì •
seed_everything(CFG['SEEDS'][0])

class Word2VecCYP3A4Predictor:
    def __init__(self):
        self.model = None
        self.scaler = StandardScaler()
        self.best_params = None
        # ëª¨ë“  RDKit ì„¤ëª…ì ì´ë¦„
        self.descriptor_names = [desc_name for desc_name, _ in Descriptors._descList]
        # ğŸ¯ íƒ€ê²Ÿ ë³€í™˜ ì¶”ê°€
        self.target_transformer = None
        self.use_target_transform = True
        # ğŸ¤– Word2Vec ëª¨ë¸ ë¡œë“œ
        self.word2vec_model = None
        self.load_word2vec_model()
    
    def load_word2vec_model(self):
        """ğŸ¤– Word2Vec ëª¨ë¸ ë¡œë“œ"""
        try:
            with open('model_300dim.pkl', 'rb') as f:
                self.word2vec_model = pickle.load(f)
            print("âœ… Word2Vec ëª¨ë¸ ë¡œë“œ ì„±ê³µ (300ì°¨ì›)")
        except Exception as e:
            print(f"âŒ Word2Vec ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("Word2Vec ì—†ì´ Morgan ê¸°ë°˜ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
            self.word2vec_model = None
    
    def tokenize_smiles(self, smiles):
        """ğŸ§¬ SMILESë¥¼ í† í°ìœ¼ë¡œ ë¶„ë¦¬"""
        tokens = []
        i = 0
        while i < len(smiles):
            # ë‘ ê¸€ì í† í°ë“¤ ë¨¼ì € í™•ì¸
            if i < len(smiles) - 1:
                two_char = smiles[i:i+2]
                if two_char in ['Cl', 'Br', 'Si', 'Na', 'Mg', 'Al', 'Ca', 'Fe', 'Cu', 'Zn']:
                    tokens.append(two_char)
                    i += 2
                    continue
            
            # í•œ ê¸€ì í† í°
            tokens.append(smiles[i])
            i += 1
        
        return tokens
    
    def smiles_to_word2vec(self, smiles):
        """ğŸ¤– SMILESë¥¼ Word2Vec ë²¡í„°ë¡œ ë³€í™˜"""
        if self.word2vec_model is None:
            return np.zeros(300)  # ê¸°ë³¸ 300ì°¨ì› ì˜ë²¡í„°
        
        tokens = self.tokenize_smiles(smiles)
        vectors = []
        
        for token in tokens:
            try:
                # Word2Vec ë²¡í„° ì¶”ì¶œ
                vector = self.word2vec_model.wv[token]
                vectors.append(vector)
            except (KeyError, AttributeError):
                # í† í°ì´ ì—†ìœ¼ë©´ ë¬´ì‹œ
                continue
        
        if vectors:
            # í‰ê·  ë²¡í„° ê³„ì‚° (ë¶„ì ì „ì²´ í‘œí˜„)
            return np.mean(vectors, axis=0)
        else:
            # ë²¡í„°ê°€ ì—†ìœ¼ë©´ ì˜ë²¡í„°
            return np.zeros(300)

    def smiles_to_features(self, smiles):
        """ğŸ¤– Word2Vec + Morgan ì¡°í•© íŠ¹ì„± ì¶”ì¶œ"""
        mol = Chem.MolFromSmiles(smiles)
        if mol is None:
            return None
        
        # ğŸ¤– 1. Word2Vec ë²¡í„° (300ì°¨ì›) - í•µì‹¬ í˜ì‹ !
        word2vec_features = self.smiles_to_word2vec(smiles)
        
        # ğŸ§¬ 2. Morgan Fingerprint (ë‹¨ì¼ radius=2, 1024 bits)
        fp_morgan = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=1024)
        arr_morgan = np.zeros((1024,))
        DataStructs.ConvertToNumpyArray(fp_morgan, arr_morgan)
        
        # 3. MACCS Keys (167 bits)
        fp_maccs = AllChem.GetMACCSKeysFingerprint(mol)
        arr_maccs = np.zeros((167,))
        DataStructs.ConvertToNumpyArray(fp_maccs, arr_maccs)
        
        # ğŸ¯ 4. í•µì‹¬ RDKit Descriptors (12ê°œ)
        core_descriptors = []
        important_descs = [
            'MolWt', 'MolLogP', 'NumHDonors', 'NumHAcceptors', 'TPSA',
            'NumRotatableBonds', 'NumAromaticRings', 'NumAliphaticRings',
            'NumHeteroatoms', 'FractionCsp3', 'LabuteASA', 'BertzCT'
        ]
        
        for desc_name in important_descs:
            try:
                desc_func = getattr(Descriptors, desc_name)
                desc_value = desc_func(mol)
                core_descriptors.append(desc_value if desc_value is not None else 0)
            except:
                core_descriptors.append(0)
        
        # ğŸ¤– ëª¨ë“  íŠ¹ì„± ê²°í•©: Word2Vec + Morgan + MACCS + Descriptors
        features = np.concatenate([word2vec_features, arr_morgan, arr_maccs, core_descriptors])
        return features

    def prepare_data(self, df, is_training=True):
        """ğŸ“Š ë°ì´í„° ì „ì²˜ë¦¬ + íƒ€ê²Ÿ ë³€í™˜"""
        smiles_col = 'Canonical_Smiles' if 'Canonical_Smiles' in df.columns else 'SMILES'
        
        print(f"íŠ¹ì„± ì¶”ì¶œ ì¤‘... ({len(df)}ê°œ ë¶„ì)")
        features_list = []
        valid_indices = []
        
        for idx, smiles in enumerate(df[smiles_col]):
            features = self.smiles_to_features(smiles)
            if features is not None:
                features_list.append(features)
                valid_indices.append(idx)
            
            if (idx + 1) % 100 == 0:
                print(f"  ì§„í–‰: {idx + 1}/{len(df)}")
        
        if not features_list:
            raise ValueError("ìœ íš¨í•œ ë¶„ìê°€ ì—†ìŠµë‹ˆë‹¤!")
        
        X = np.array(features_list)
        print(f"âœ… íŠ¹ì„± ì¶”ì¶œ ì™„ë£Œ: {X.shape[1]:,}ê°œ íŠ¹ì„±")
        
        # NaN/Inf ì²˜ë¦¬
        X = np.nan_to_num(X, nan=0, posinf=0, neginf=0)
        
        # ìŠ¤ì¼€ì¼ë§
        if is_training:
            X = self.scaler.fit_transform(X)
        else:
            X = self.scaler.transform(X)
        
        return X, valid_indices
    
    def transform_target(self, y, fit=False):
        """ğŸ¯ íƒ€ê²Ÿ ë³€í™˜ (sqrtë¡œ ë¶„í¬ ê°œì„ )"""
        if not self.use_target_transform:
            return y
            
        if fit:
            # sqrt ë³€í™˜ (0ì— ê°€ê¹Œìš´ ê°’ë“¤ ì²˜ë¦¬)
            y_transformed = np.sqrt(y + 1)  # +1ë¡œ 0 ì²˜ë¦¬
            self.target_mean = np.mean(y_transformed)
            self.target_std = np.std(y_transformed)
            return y_transformed
        else:
            # ê¸°ì¡´ ë³€í™˜ ì ìš©
            return np.sqrt(y + 1)
    
    def inverse_transform_target(self, y_transformed):
        """ğŸ¯ íƒ€ê²Ÿ ì—­ë³€í™˜"""
        if not self.use_target_transform:
            return y_transformed
            
        # sqrt ì—­ë³€í™˜
        y_original = np.square(y_transformed) - 1
        return np.clip(y_original, 0, 100)  # ë²”ìœ„ ë³´ì •

    def get_score(self, y_true, y_pred):
        """ë¦¬ë”ë³´ë“œ ìŠ¤ì½”ì–´ ê³„ì‚°: 0.5 * (1 - NRMSE) + 0.5 * Pearson_Correlation"""
        # RMSE ê³„ì‚°
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        
        # Normalized RMSE
        y_range = y_true.max() - y_true.min()
        nrmse = rmse / y_range if y_range > 0 else 0
        
        # Pearson ìƒê´€ê´€ê³„
        correlation = np.corrcoef(y_true, y_pred)[0, 1]
        if np.isnan(correlation):
            correlation = 0
        
        # ìµœì¢… ìŠ¤ì½”ì–´
        score = 0.5 * (1 - nrmse) + 0.5 * correlation
        return score

    def objective(self, trial, X, y):
        """ğŸ¤– Word2Vec + ê²¬ê³ ì„± Optuna ìµœì í™”"""
        # ğŸ¯ íƒ€ê²Ÿ ë³€í™˜ ì ìš©
        y_transformed = self.transform_target(y, fit=True)
        
        # ğŸ¯ ì•½ê°„ ë” ê³µê²©ì ì¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° (0.85+ ëª©í‘œ)
        params = {
            'objective': 'regression',
            'metric': 'rmse',
            'verbose': -1,
            'n_jobs': -1,
            'seed': CFG['SEEDS'][0],
            'boosting_type': 'gbdt',
            'n_estimators': trial.suggest_int('n_estimators', 800, 2000),  # ë²”ìœ„ í™•ëŒ€
            'learning_rate': trial.suggest_float('learning_rate', 0.03, 0.12),  # ë²”ìœ„ í™•ëŒ€
            'num_leaves': trial.suggest_int('num_leaves', 15, 80),  # ë²”ìœ„ í™•ëŒ€
            'max_depth': trial.suggest_int('max_depth', 4, 10),  # ë²”ìœ„ í™•ëŒ€
            'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 0.95),
            'bagging_fraction': trial.suggest_float('bagging_fraction', 0.7, 0.95),
            'bagging_freq': trial.suggest_int('bagging_freq', 3, 7),
            'min_child_samples': trial.suggest_int('min_child_samples', 15, 45),  # ë²”ìœ„ ì¡°ì •
            'lambda_l1': trial.suggest_float('lambda_l1', 0.001, 0.5),  # ì •ê·œí™” ì™„í™”
            'lambda_l2': trial.suggest_float('lambda_l2', 0.001, 0.5),
        }
        
        # K-í´ë“œ êµì°¨ ê²€ì¦
        kf = KFold(n_splits=CFG['N_SPLITS'], shuffle=True, random_state=CFG['SEEDS'][0])
        all_y_true = []
        all_y_pred = []
        
        for train_idx, val_idx in kf.split(X, y_transformed):
            X_train, X_val = X[train_idx], X[val_idx]
            y_train, y_val = y_transformed[train_idx], y_transformed[val_idx]
            
            # ëª¨ë¸ í›ˆë ¨
            model = lgb.LGBMRegressor(**params)
            model.fit(
                X_train, y_train,
                eval_set=[(X_val, y_val)],
                eval_metric='rmse',
                callbacks=[lgb.early_stopping(50, verbose=False)]
            )
            
            # ì˜ˆì¸¡ ë° ì—­ë³€í™˜
            y_pred_transformed = model.predict(X_val)
            y_pred = self.inverse_transform_target(y_pred_transformed)
            y_true = y[val_idx]  # ì›ë³¸ íƒ€ê²Ÿ ì‚¬ìš©
            
            all_y_true.extend(y_true)
            all_y_pred.extend(y_pred)
        
        all_y_true = np.array(all_y_true)
        all_y_pred = np.array(all_y_pred)
        
        # ğŸ² ë¬´ì‘ìœ„ 80% ì‹œë®¬ë ˆì´ì…˜ ìŠ¤ì½”ì–´ (ë¦¬ë”ë³´ë“œ ëª¨ë°©)
        if CFG['SIMULATE_80_PERCENT']:
            robust_score, score_std = simulate_random_80_percent(
                all_y_true, all_y_pred, CFG['N_SIMULATIONS']
            )
            # ì•ˆì •ì„± ë³´ë„ˆìŠ¤: í‘œì¤€í¸ì°¨ê°€ ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
            stability_bonus = max(0, 0.1 - score_std)
            return robust_score + stability_bonus
        else:
            # ê¸°ë³¸ ìŠ¤ì½”ì–´
            return self.get_score(all_y_true, all_y_pred)

    def train(self, X_train, y_train):
        """ğŸ¤– Word2Vec ê¸°ë°˜ ëª¨ë¸ í›ˆë ¨ (ë¶„ì ì‹œí€€ìŠ¤ í•™ìŠµ + Optuna)"""
        print("\nğŸ¤– Word2Vec ê¸°ë°˜ Optuna ìµœì í™”...")
        print("ğŸ¤– SMILES ì‹œí€€ìŠ¤ íŒ¨í„´ í•™ìŠµ (300ì°¨ì›)")
        print("ğŸ§¬ Morgan + MACCS + í•µì‹¬ Descriptors")
        print("ğŸ¯ íƒ€ê²Ÿ ë³€í™˜ (sqrt) ì ìš©")
        if CFG['SIMULATE_80_PERCENT']:
            print(f"ğŸ² ë¬´ì‘ìœ„ 80% ìƒ˜í”Œë§ ì‹œë®¬ë ˆì´ì…˜ í™œì„±í™” ({CFG['N_SIMULATIONS']}íšŒ)")
        
        # Optuna ìµœì í™”
        study = optuna.create_study(direction='maximize', study_name='word2vec_lgbm')
        study.optimize(lambda trial: self.objective(trial, X_train, y_train), n_trials=CFG['N_TRIALS'])
        
        print(f"âœ… Word2Vec ìµœì í™” ì™„ë£Œ! ìµœê³  ìŠ¤ì½”ì–´: {study.best_value:.4f}")
        print(f"ìµœì  íŒŒë¼ë¯¸í„°: {study.best_params}")
        
        # ìµœì  íŒŒë¼ë¯¸í„° ì €ì¥
        self.best_params = {
            'objective': 'regression',
            'metric': 'rmse',
            'verbose': -1,
            'n_jobs': -1,
            'seed': CFG['SEEDS'][0],
            'boosting_type': 'gbdt',
            'n_estimators': 1500  # ê¸°ë³¸ê°’
        }
            
        self.best_params.update(study.best_params)
        
        return study.best_value

    def predict(self, X_test):
        """ğŸ¤– Word2Vec + ë‹¤ì¤‘ ì‹œë“œ ê²¬ê³  ì•™ìƒë¸” ì˜ˆì¸¡"""
        print("ğŸ¤– Word2Vec ê²¬ê³  ì•™ìƒë¸” ì˜ˆì¸¡...")
        print(f"ğŸ”¢ ì‹œë“œ ìˆ˜: {len(CFG['SEEDS'])}, í´ë“œ ìˆ˜: {CFG['N_SPLITS']}")
        print(f"ğŸ¯ ì´ ëª¨ë¸ ìˆ˜: {len(CFG['SEEDS']) * CFG['N_SPLITS']}")
        
        # í›ˆë ¨ ë°ì´í„°
        X_train = self.X_train_stored
        y_train = self.y_train_stored
        y_train_transformed = self.transform_target(y_train, fit=False)  # íƒ€ê²Ÿ ë³€í™˜
        
        all_predictions = []
        
        # ë‹¤ì¤‘ ì‹œë“œ ì•™ìƒë¸”
        for seed_idx, seed in enumerate(CFG['SEEDS']):
            print(f"\nğŸ”„ ì‹œë“œ {seed} ({seed_idx + 1}/{len(CFG['SEEDS'])}) ì²˜ë¦¬ ì¤‘...")
            
            # ì‹œë“œë³„ K-í´ë“œ
            kf = KFold(n_splits=CFG['N_SPLITS'], shuffle=True, random_state=seed)
            seed_preds = np.zeros(len(X_test))
            
            for fold, (train_idx, _) in enumerate(kf.split(X_train, y_train_transformed)):
                if fold % 5 == 0:  # 5ê°œ í´ë“œë§ˆë‹¤ ì¶œë ¥
                    print(f"  ì‹œë“œ {seed} - í´ë“œ {fold + 1}/{CFG['N_SPLITS']}")
                
                X_fold_train = X_train[train_idx]
                y_fold_train = y_train_transformed[train_idx]  # ë³€í™˜ëœ íƒ€ê²Ÿ ì‚¬ìš©
                
                # ëª¨ë¸ í›ˆë ¨ (ì‹œë“œë³„ íŒŒë¼ë¯¸í„°)
                params = self.best_params.copy()
                params['seed'] = seed
                
                model = lgb.LGBMRegressor(**params)
                model.fit(X_fold_train, y_fold_train)
                
                # í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ (ë³€í™˜ëœ íƒ€ê²Ÿ ê³µê°„)
                fold_pred_transformed = model.predict(X_test)
                # ì—­ë³€í™˜
                fold_pred = self.inverse_transform_target(fold_pred_transformed)
                seed_preds += fold_pred / CFG['N_SPLITS']
            
            all_predictions.append(seed_preds)
        
        # ğŸ›¡ï¸ ê²¬ê³ í•œ ì•™ìƒë¸”: ëª¨ë“  ì‹œë“œ ì˜ˆì¸¡ì˜ í‰ê· 
        final_predictions = np.mean(all_predictions, axis=0)
        
        # ğŸ“Š ì˜ˆì¸¡ ìŠ¤ë¬´ë”© (ê·¹ê°’ ë°©ì§€)
        final_predictions = self.smooth_predictions(final_predictions)
        
        print(f"âœ… Word2Vec ê²¬ê³  ì•™ìƒë¸” ì™„ë£Œ! ({len(CFG['SEEDS']) * CFG['N_SPLITS']}ê°œ ëª¨ë¸)")
        return final_predictions
    
    def smooth_predictions(self, predictions):
        """ğŸ“Š ì˜ˆì¸¡ê°’ ìŠ¤ë¬´ë”© (ê·¹ê°’ ë°©ì§€)"""
        # 1. ê¸°ë³¸ í´ë¦¬í•‘
        predictions = np.clip(predictions, 0, 100)
        
        # 2. ë¶€ë“œëŸ¬ìš´ ì¡°ì • (ê·¹ê°’ ì–µì œ)
        mean_pred = np.mean(predictions)
        std_pred = np.std(predictions)
        
        # 3Ïƒ ì´ìƒ ê·¹ê°’ë“¤ì„ ë¶€ë“œëŸ½ê²Œ ì¡°ì •
        upper_bound = mean_pred + 2.5 * std_pred
        lower_bound = mean_pred - 2.5 * std_pred
        
        predictions = np.where(predictions > upper_bound, 
                              upper_bound + 0.3 * (predictions - upper_bound),
                              predictions)
        predictions = np.where(predictions < lower_bound,
                              lower_bound + 0.3 * (predictions - lower_bound), 
                              predictions)
        
        # ìµœì¢… í´ë¦¬í•‘
        return np.clip(predictions, 0, 100)

    def store_training_data(self, X_train, y_train):
        """í›ˆë ¨ ë°ì´í„° ì €ì¥ (ì˜ˆì¸¡ ì‹œ ì‚¬ìš©)"""
        self.X_train_stored = X_train
        self.y_train_stored = y_train

def main():
    print("ğŸ¤– Word2Vec ê¸°ë°˜ CYP3A4 ì˜ˆì¸¡ ëª¨ë¸ (0.85+ ëª©í‘œ) ğŸ¤–")
    print("=" * 70)
    print("ğŸ¯ ì „ëµ: Word2Vec ë¶„ìí‘œí˜„ + Morgan + ê²¬ê³  ì•™ìƒë¸”")
    print("ğŸ¤– SMILES Word2Vec ë²¡í„° (300ì°¨ì›)")
    print("ğŸ§¬ Morgan Fingerprint + MACCS + í•µì‹¬ Descriptors")
    print("ğŸ“Š íƒ€ê²Ÿ ë³€í™˜ (sqrt) + ë‹¤ì¤‘ ì‹œë“œ ì•™ìƒë¸”")
    print("ğŸ² ë¬´ì‘ìœ„ 80% ìƒ˜í”Œë§ ëŒ€ì‘")
    print("âš¡ ë¶„ì ì‹œí€€ìŠ¤ íŒ¨í„´ í•™ìŠµìœ¼ë¡œ 0.85+ ë„ì „!")
    
    try:
        # ë°ì´í„° ë¡œë“œ
        print("\nğŸ“ ë°ì´í„° ë¡œë“œ...")
        train_df = pd.read_csv('data/train.csv')
        test_df = pd.read_csv('data/test.csv')
        sample_submission = pd.read_csv('data/sample_submission.csv')
        
        print(f"í›ˆë ¨: {len(train_df)}ê°œ, í…ŒìŠ¤íŠ¸: {len(test_df)}ê°œ")
        print(f"Inhibition ë²”ìœ„: {train_df['Inhibition'].min():.1f} ~ {train_df['Inhibition'].max():.1f}")
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        predictor = Word2VecCYP3A4Predictor()
        
        # íŠ¹ì„± ì¶”ì¶œ
        print("\nğŸ”¬ íŠ¹ì„± ì¶”ì¶œ...")
        X_train, train_valid_idx = predictor.prepare_data(train_df, is_training=True)
        X_test, test_valid_idx = predictor.prepare_data(test_df, is_training=False)
        
        # ìœ íš¨í•œ í›ˆë ¨ ë°ì´í„°ë§Œ ì‚¬ìš©
        y_train = train_df.iloc[train_valid_idx]['Inhibition'].values
        
        print(f"âœ… ìµœì¢… í›ˆë ¨ ë°ì´í„°: {X_train.shape}")
        print(f"âœ… ìµœì¢… í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape}")
        
        # í›ˆë ¨ ë°ì´í„° ì €ì¥
        predictor.store_training_data(X_train, y_train)
        
        # ëª¨ë¸ í›ˆë ¨
        print("\nğŸ¯ ëª¨ë¸ í›ˆë ¨...")
        best_score = predictor.train(X_train, y_train)
        
        # ì˜ˆì¸¡
        print("\nğŸš€ ìµœì¢… ì˜ˆì¸¡...")
        predictions = predictor.predict(X_test)
        
        # ì˜ˆì¸¡ í›„ì²˜ë¦¬ (ë²”ìœ„ ì œí•œ)
        predictions = np.clip(predictions, 0, 100)
        
        # ì œì¶œ íŒŒì¼ ìƒì„±
        submission = sample_submission.copy()
        submission.iloc[test_valid_idx, submission.columns.get_loc('Inhibition')] = predictions
        
        # ì˜ˆì¸¡í•˜ì§€ ëª»í•œ ë¶€ë¶„ì€ í‰ê· ê°’ìœ¼ë¡œ ì±„ì›€
        mean_inhibition = train_df['Inhibition'].mean()
        submission['Inhibition'].fillna(mean_inhibition, inplace=True)
        
        # ì €ì¥
        output_file = 'submission_word2vec.csv'
        submission.to_csv(output_file, index=False)
        
        print(f"\nâœ… Word2Vec ê¸°ë°˜ ì œì¶œ íŒŒì¼ ì €ì¥: {output_file}")
        print(f"ğŸ¤– ìµœê³  Word2Vec ìŠ¤ì½”ì–´: {best_score:.4f}")
        print(f"ğŸ”¢ ì´ ì•™ìƒë¸” ëª¨ë¸ ìˆ˜: {len(CFG['SEEDS']) * CFG['N_SPLITS']}ê°œ")
        print(f"\nğŸ“Š Word2Vec ì˜ˆì¸¡ ê²°ê³¼ ìš”ì•½:")
        print(submission['Inhibition'].describe())
        
        print(f"\nğŸ¤– Word2Vec ê¸°ë°˜ ëª¨ë¸ ì™„ë£Œ!")
        print(f"ğŸ¤– SMILES ì‹œí€€ìŠ¤ íŒ¨í„´ í•™ìŠµ (300ì°¨ì›)")
        print(f"ğŸ§¬ Morgan + MACCS + í•µì‹¬ Descriptors ë³´ì™„")
        print(f"ğŸ“Š íƒ€ê²Ÿ ë³€í™˜ìœ¼ë¡œ ë¶„í¬ ìµœì í™”")
        print(f"ğŸ›¡ï¸ ê²¬ê³  ì•™ìƒë¸”ë¡œ ì•ˆì •ì„± ìœ ì§€")
        print(f"âš¡ ê¸°ëŒ€ íš¨ê³¼: ë¶„ì í‘œí˜„ í˜ì‹ ìœ¼ë¡œ 0.85+ ë‹¬ì„±!")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 