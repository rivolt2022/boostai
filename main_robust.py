import pandas as pd
import numpy as np
import os
import random
import pickle
import warnings
from rdkit import Chem
from rdkit.Chem import AllChem, DataStructs, Descriptors, rdMolDescriptors
from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, QuantileTransformer
from sklearn.impute import SimpleImputer
from sklearn.model_selection import KFold, RepeatedKFold, StratifiedKFold, ShuffleSplit
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor
from sklearn.linear_model import Ridge, ElasticNet, BayesianRidge, HuberRegressor
import lightgbm as lgb
import xgboost as xgb
import catboost as cb
import optuna
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

warnings.filterwarnings('ignore')

# ğŸ›¡ï¸ ë¬´ì‘ìœ„ 80% ìƒ˜í”Œë§ ê²¬ê³ ì„±ì— íŠ¹í™”ëœ ì„¤ì •
CFG = {
    'NBITS': 2048,
    'SEEDS': [42, 123, 456, 789, 999],  # 5ê°œ ì‹œë“œë¡œ ê· í˜•
    'N_SPLITS': 10,        # ìµœì í™” ì‹œ ë¹ ë¥¸ ì‹¤í–‰
    'N_REPEATS': 2,        # ë°˜ë³µ êµì°¨ê²€ì¦
    'OPTIMIZATION_TRIALS': 100,  # ğŸ”¥ Optuna ìµœì í™” ì‹œí–‰ ìˆ˜
    'ENSEMBLE_TRIALS': 200,      # ì•™ìƒë¸” ì‹œ ë” ë§ì€ ì‹œí–‰
    'ENABLE_OPTIMIZATION': True,  # ğŸ¯ Optuna ìµœì í™” í™œì„±í™”
    'OPTIMIZATION_TIMEOUT': 3600,  # 1ì‹œê°„ ìµœì í™” íƒ€ì„ì•„ì›ƒ
    'RANDOM_SAMPLING_WEIGHT': 0.8,  # ë¬´ì‘ìœ„ ìƒ˜í”Œë§ ê°€ì¤‘ì¹˜
    'STABILITY_WEIGHT': 0.2,         # ì•ˆì •ì„± ê°€ì¤‘ì¹˜
}

def seed_everything(seed):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)

class OptimizedRobustPredictor:
    def __init__(self):
        self.models = {}
        self.scalers = {}
        self.imputer = SimpleImputer(strategy='median')
        self.optimized_params = {}
        
    def get_core_descriptors(self, mol):
        """í•µì‹¬ ë¶„ì ì„¤ëª…ì"""
        try:
            desc_dict = {
                'MolWt': Descriptors.MolWt(mol),
                'LogP': Descriptors.MolLogP(mol),
                'TPSA': Descriptors.TPSA(mol),
                'NumHDonors': Descriptors.NumHDonors(mol),
                'NumHAcceptors': Descriptors.NumHAcceptors(mol),
                'NumRotatableBonds': Descriptors.NumRotatableBonds(mol),
                'NumAromaticRings': Descriptors.NumAromaticRings(mol),
                'NumSaturatedRings': Descriptors.NumSaturatedRings(mol),
                'NumAliphaticRings': Descriptors.NumAliphaticRings(mol),
                'HeavyAtomCount': Descriptors.HeavyAtomCount(mol),
                'RingCount': Descriptors.RingCount(mol),
                'BertzCT': Descriptors.BertzCT(mol),
            }
            return desc_dict
        except:
            return {key: 0 for key in ['MolWt', 'LogP', 'TPSA', 'NumHDonors', 'NumHAcceptors', 
                                      'NumRotatableBonds', 'NumAromaticRings', 'NumSaturatedRings',
                                      'NumAliphaticRings', 'HeavyAtomCount', 'RingCount', 'BertzCT']}

    def smiles_to_robust_features(self, smiles):
        """ê²¬ê³ í•œ íŠ¹ì„± ì¶”ì¶œ"""
        mol = Chem.MolFromSmiles(smiles)
        if mol is None:
            return None

        try:
            # 1. Multiple Morgan Fingerprints
            morgan_features = []
            for radius in [1, 2, 3]:
                fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=CFG['NBITS'])
                arr = np.zeros((CFG['NBITS'],))
                DataStructs.ConvertToNumpyArray(fp, arr)
                morgan_features.append(arr)

            # 2. MACCS Keys
            fp_maccs = AllChem.GetMACCSKeysFingerprint(mol)
            arr_maccs = np.zeros((167,))
            DataStructs.ConvertToNumpyArray(fp_maccs, arr_maccs)

            # 3. í•µì‹¬ ë¶„ì ì„¤ëª…ì
            descriptors = self.get_core_descriptors(mol)

            # 4. CYP3A4 í•µì‹¬ êµ¬ì¡° ì•Œë¦¼
            structural_alerts = {
                'HasBenzene': int(mol.HasSubstructMatch(Chem.MolFromSmarts('c1ccccc1'))),
                'HasPyridine': int(mol.HasSubstructMatch(Chem.MolFromSmarts('c1ccncc1'))),
                'HasImidazole': int(mol.HasSubstructMatch(Chem.MolFromSmarts('c1cnc[nH]1'))),
                'HasAmide': int(mol.HasSubstructMatch(Chem.MolFromSmarts('C(=O)N'))),
                'HasEster': int(mol.HasSubstructMatch(Chem.MolFromSmarts('C(=O)O'))),
                'HasFluorine': int(mol.HasSubstructMatch(Chem.MolFromSmarts('[F]'))),
                'HasChlorine': int(mol.HasSubstructMatch(Chem.MolFromSmarts('[Cl]'))),
                'HasNitro': int(mol.HasSubstructMatch(Chem.MolFromSmarts('[N+](=O)[O-]'))),
                'HasTrifluoromethyl': int(mol.HasSubstructMatch(Chem.MolFromSmarts('C(F)(F)F'))),
                'HasIndole': int(mol.HasSubstructMatch(Chem.MolFromSmarts('c1ccc2[nH]ccc2c1'))),
            }

            return morgan_features, arr_maccs, descriptors, structural_alerts
            
        except Exception as e:
            return None

    def prepare_robust_data(self, df, is_training=True):
        """ê²¬ê³ í•œ ë°ì´í„° ì¤€ë¹„"""
        print("ê²¬ê³ í•œ íŠ¹ì„± ì¶”ì¶œ ì¤‘...")
        
        all_features = []
        failed_count = 0
        
        for i, smiles in enumerate(df['Canonical_Smiles']):
            if i % 200 == 0:
                print(f"ì²˜ë¦¬ ì¤‘: {i}/{len(df)}")
            
            result = self.smiles_to_robust_features(smiles)
            
            if result is None:
                failed_count += 1
                morgan_features = [np.zeros(CFG['NBITS']) for _ in range(3)]
                arr_maccs = np.zeros(167)
                descriptors = {key: 0 for key in ['MolWt', 'LogP', 'TPSA', 'NumHDonors', 'NumHAcceptors', 
                                                 'NumRotatableBonds', 'NumAromaticRings', 'NumSaturatedRings',
                                                 'NumAliphaticRings', 'HeavyAtomCount', 'RingCount', 'BertzCT']}
                structural_alerts = {key: 0 for key in ['HasBenzene', 'HasPyridine', 'HasImidazole', 
                                                       'HasAmide', 'HasEster', 'HasFluorine', 
                                                       'HasChlorine', 'HasNitro', 'HasTrifluoromethyl', 'HasIndole']}
            else:
                morgan_features, arr_maccs, descriptors, structural_alerts = result

            all_features.append((morgan_features, arr_maccs, list(descriptors.values()), 
                               list(structural_alerts.values())))

        # íŠ¹ì„± ê²°í•©
        morgan_all = np.hstack([np.array([item[0][i] for item in all_features]) for i in range(3)])
        maccs_all = np.array([item[1] for item in all_features])
        desc_all = np.array([item[2] for item in all_features])
        alert_all = np.array([item[3] for item in all_features])

        X = np.hstack([morgan_all, maccs_all, desc_all, alert_all])
        X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)
        
        if is_training:
            X = self.imputer.fit_transform(X)
        else:
            X = self.imputer.transform(X)
        
        print(f"âœ… ê²¬ê³ í•œ íŠ¹ì„± ì¶”ì¶œ ì™„ë£Œ: {X.shape[1]:,}ê°œ íŠ¹ì„±")
        return X

    def get_leaderboard_score(self, y_true, y_pred):
        """ì •í™•í•œ ë¦¬ë”ë³´ë“œ í‰ê°€ ì§€í‘œ"""
        mse = mean_squared_error(y_true, y_pred)
        rmse = np.sqrt(mse)
        y_range = np.max(y_true) - np.min(y_true)
        normalized_rmse = rmse / y_range
        A = min(normalized_rmse, 1)
        
        correlation = np.corrcoef(y_true, y_pred)[0, 1]
        if np.isnan(correlation):
            correlation = 0
        B = np.clip(correlation, 0, 1)
        
        score = 0.5 * (1 - A) + 0.5 * B
        return score, A, B, correlation

    def simulate_random_80_percent_cv(self, y_true, y_pred, n_simulations=100):
        """ğŸ¯ êµì°¨ê²€ì¦ìš© ë¹ ë¥¸ ë¬´ì‘ìœ„ 80% ìƒ˜í”Œë§ ì‹œë®¬ë ˆì´ì…˜"""
        scores = []
        indices = np.arange(len(y_true))
        
        for _ in range(n_simulations):
            sample_size = int(len(indices) * 0.8)
            random_indices = np.random.choice(indices, size=sample_size, replace=False)
            
            y_true_sample = y_true.iloc[random_indices] if hasattr(y_true, 'iloc') else y_true[random_indices]
            y_pred_sample = y_pred[random_indices]
            
            score, _, _, _ = self.get_leaderboard_score(y_true_sample, y_pred_sample)
            scores.append(score)
        
        return np.mean(scores), np.std(scores)

    def objective_lgb(self, trial, X, y):
        """ğŸ”¥ LightGBM ë¬´ì‘ìœ„ 80% ìƒ˜í”Œë§ ìµœì í™” ëª©ì í•¨ìˆ˜"""
        params = {
            'objective': 'regression',
            'metric': 'rmse',
            'verbose': -1,
            'n_jobs': -1,
            'seed': trial.suggest_categorical('seed', CFG['SEEDS']),
            'boosting_type': 'gbdt',
            'n_estimators': trial.suggest_int('n_estimators', 1000, 4000),
            'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.05, log=True),
            'num_leaves': trial.suggest_int('num_leaves', 8, 40),
            'max_depth': trial.suggest_int('max_depth', 3, 10),
            'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 0.9),
            'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 0.9),
            'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),
            'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),
            'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),
            'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),
            'min_split_gain': trial.suggest_float('min_split_gain', 1e-8, 1.0, log=True),
        }

        # ğŸ¯ ë¬´ì‘ìœ„ 80% ìƒ˜í”Œë§ ê²¬ê³ ì„± í‰ê°€
        cv_scores = []
        random_sampling_scores = []
        
        # ë‹¤ì–‘í•œ CV ì „ëµìœ¼ë¡œ ê²¬ê³ ì„± í™•ì¸
        cv_strategies = [
            KFold(n_splits=CFG['N_SPLITS'], shuffle=True, random_state=42),
            RepeatedKFold(n_splits=5, n_repeats=CFG['N_REPEATS'], random_state=42),
            ShuffleSplit(n_splits=8, test_size=0.2, random_state=42)
        ]
        
        for cv_strategy in cv_strategies:
            for train_idx, val_idx in cv_strategy.split(X, y):
                X_train, X_val = X[train_idx], X[val_idx]
                y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

                scaler = RobustScaler()
                X_train_scaled = scaler.fit_transform(X_train)
                X_val_scaled = scaler.transform(X_val)

                model = lgb.LGBMRegressor(**params)
                model.fit(X_train_scaled, y_train, eval_set=[(X_val_scaled, y_val)],
                          callbacks=[lgb.early_stopping(200, verbose=False)])
                
                y_pred = model.predict(X_val_scaled)
                
                # ê¸°ë³¸ ìŠ¤ì½”ì–´
                score, _, _, _ = self.get_leaderboard_score(y_val, y_pred)
                cv_scores.append(score)
                
                # ğŸ¯ ë¬´ì‘ìœ„ 80% ìƒ˜í”Œë§ ì‹œë®¬ë ˆì´ì…˜
                random_mean, random_std = self.simulate_random_80_percent_cv(y_val, y_pred)
                random_sampling_scores.append(random_mean)
                
                # ì‹œê°„ ì ˆì•½ì„ ìœ„í•´ ì¼ë¶€ë§Œ í‰ê°€
                if len(cv_scores) >= 10:
                    break
            if len(cv_scores) >= 10:
                break

        # ğŸ›¡ï¸ ê²¬ê³ ì„± ì ìˆ˜ ê³„ì‚°
        base_score = np.mean(cv_scores)
        random_score = np.mean(random_sampling_scores)
        stability_score = 1.0 / (1.0 + np.std(cv_scores))  # ì•ˆì •ì„± ì ìˆ˜
        
        # ğŸ¯ ìµœì¢… ëª©ì í•¨ìˆ˜: ë¬´ì‘ìœ„ ìƒ˜í”Œë§ ì„±ëŠ¥ + ì•ˆì •ì„±
        final_score = (CFG['RANDOM_SAMPLING_WEIGHT'] * random_score + 
                      CFG['STABILITY_WEIGHT'] * stability_score)
        
        return final_score

    def objective_xgb(self, trial, X, y):
        """ğŸ”¥ XGBoost ë¬´ì‘ìœ„ 80% ìƒ˜í”Œë§ ìµœì í™” ëª©ì í•¨ìˆ˜"""
        params = {
            'n_estimators': trial.suggest_int('n_estimators', 1000, 4000),
            'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.05, log=True),
            'max_depth': trial.suggest_int('max_depth', 3, 10),
            'subsample': trial.suggest_float('subsample', 0.4, 0.9),
            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.4, 0.9),
            'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),
            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),
            'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),
            'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),
            'random_state': trial.suggest_categorical('random_state', CFG['SEEDS']),
            'n_jobs': -1,
        }

        cv_scores = []
        random_sampling_scores = []
        
        kf = KFold(n_splits=5, shuffle=True, random_state=42)
        for train_idx, val_idx in kf.split(X, y):
            X_train, X_val = X[train_idx], X[val_idx]
            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

            scaler = RobustScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_val_scaled = scaler.transform(X_val)

            model = xgb.XGBRegressor(**params)
            try:
                model.fit(X_train_scaled, y_train, eval_set=[(X_val_scaled, y_val)],
                         callbacks=[xgb.callback.EarlyStopping(rounds=200)])
            except:
                model.fit(X_train_scaled, y_train)
            
            y_pred = model.predict(X_val_scaled)
            
            score, _, _, _ = self.get_leaderboard_score(y_val, y_pred)
            cv_scores.append(score)
            
            random_mean, _ = self.simulate_random_80_percent_cv(y_val, y_pred)
            random_sampling_scores.append(random_mean)

        base_score = np.mean(cv_scores)
        random_score = np.mean(random_sampling_scores)
        stability_score = 1.0 / (1.0 + np.std(cv_scores))
        
        final_score = (CFG['RANDOM_SAMPLING_WEIGHT'] * random_score + 
                      CFG['STABILITY_WEIGHT'] * stability_score)
        
        return final_score

    def objective_rf(self, trial, X, y):
        """ğŸ”¥ RandomForest ë¬´ì‘ìœ„ 80% ìƒ˜í”Œë§ ìµœì í™” ëª©ì í•¨ìˆ˜"""
        params = {
            'n_estimators': trial.suggest_int('n_estimators', 200, 1500),
            'max_depth': trial.suggest_int('max_depth', 5, 20),
            'min_samples_split': trial.suggest_int('min_samples_split', 5, 50),
            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 2, 20),
            'max_features': trial.suggest_float('max_features', 0.4, 1.0),
            'bootstrap': trial.suggest_categorical('bootstrap', [True, False]),
            'random_state': trial.suggest_categorical('random_state', CFG['SEEDS']),
            'n_jobs': -1,
        }

        cv_scores = []
        random_sampling_scores = []
        
        kf = KFold(n_splits=5, shuffle=True, random_state=42)
        for train_idx, val_idx in kf.split(X, y):
            X_train, X_val = X[train_idx], X[val_idx]
            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

            scaler = RobustScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_val_scaled = scaler.transform(X_val)

            model = RandomForestRegressor(**params)
            model.fit(X_train_scaled, y_train)
            
            y_pred = model.predict(X_val_scaled)
            
            score, _, _, _ = self.get_leaderboard_score(y_val, y_pred)
            cv_scores.append(score)
            
            random_mean, _ = self.simulate_random_80_percent_cv(y_val, y_pred)
            random_sampling_scores.append(random_mean)

        base_score = np.mean(cv_scores)
        random_score = np.mean(random_sampling_scores)
        stability_score = 1.0 / (1.0 + np.std(cv_scores))
        
        final_score = (CFG['RANDOM_SAMPLING_WEIGHT'] * random_score + 
                      CFG['STABILITY_WEIGHT'] * stability_score)
        
        return final_score

    def optimize_models(self, X_train, y_train):
        """ğŸ”¥ ë‹¤ì¤‘ ëª¨ë¸ Optuna ìµœì í™”"""
        print("ğŸ”¥ Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘...")
        
        optimized_params = {}
        
        # 1. LightGBM ìµœì í™”
        print("\nğŸ¯ LightGBM ìµœì í™” ì¤‘...")
        study_lgb = optuna.create_study(direction='maximize', 
                                       sampler=optuna.samplers.TPESampler(seed=42),
                                       pruner=optuna.pruners.MedianPruner())
        study_lgb.optimize(lambda trial: self.objective_lgb(trial, X_train, y_train), 
                          n_trials=CFG['OPTIMIZATION_TRIALS'],
                          timeout=CFG['OPTIMIZATION_TIMEOUT']//3)
        
        optimized_params['lgb'] = study_lgb.best_params
        print(f"âœ… LightGBM ìµœì í™” ì™„ë£Œ - ìµœê³  ìŠ¤ì½”ì–´: {study_lgb.best_value:.4f}")
        print(f"ìµœì  íŒŒë¼ë¯¸í„°: {study_lgb.best_params}")
        
        # 2. XGBoost ìµœì í™”
        print("\nğŸ¯ XGBoost ìµœì í™” ì¤‘...")
        study_xgb = optuna.create_study(direction='maximize', 
                                       sampler=optuna.samplers.TPESampler(seed=123),
                                       pruner=optuna.pruners.MedianPruner())
        study_xgb.optimize(lambda trial: self.objective_xgb(trial, X_train, y_train), 
                          n_trials=CFG['OPTIMIZATION_TRIALS'],
                          timeout=CFG['OPTIMIZATION_TIMEOUT']//3)
        
        optimized_params['xgb'] = study_xgb.best_params
        print(f"âœ… XGBoost ìµœì í™” ì™„ë£Œ - ìµœê³  ìŠ¤ì½”ì–´: {study_xgb.best_value:.4f}")
        print(f"ìµœì  íŒŒë¼ë¯¸í„°: {study_xgb.best_params}")
        
        # 3. RandomForest ìµœì í™”
        print("\nğŸ¯ RandomForest ìµœì í™” ì¤‘...")
        study_rf = optuna.create_study(direction='maximize', 
                                      sampler=optuna.samplers.TPESampler(seed=456),
                                      pruner=optuna.pruners.MedianPruner())
        study_rf.optimize(lambda trial: self.objective_rf(trial, X_train, y_train), 
                         n_trials=CFG['OPTIMIZATION_TRIALS'],
                         timeout=CFG['OPTIMIZATION_TIMEOUT']//3)
        
        optimized_params['rf'] = study_rf.best_params
        print(f"âœ… RandomForest ìµœì í™” ì™„ë£Œ - ìµœê³  ìŠ¤ì½”ì–´: {study_rf.best_value:.4f}")
        print(f"ìµœì  íŒŒë¼ë¯¸í„°: {study_rf.best_params}")
        
        self.optimized_params = optimized_params
        
        # ìµœì í™” ê²°ê³¼ ì €ì¥
        with open('optimized_params.pkl', 'wb') as f:
            pickle.dump(optimized_params, f)
        print("\nğŸ’¾ ìµœì í™”ëœ íŒŒë¼ë¯¸í„°ê°€ 'optimized_params.pkl'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return optimized_params

    def train_optimized_ensemble(self, X_train_full, y_train_full, X_test_full):
        """ğŸ¯ ìµœì í™”ëœ íŒŒë¼ë¯¸í„°ë¡œ ê²¬ê³ í•œ ì•™ìƒë¸” í›ˆë ¨"""
        print("ğŸ¯ ìµœì í™”ëœ íŒŒë¼ë¯¸í„°ë¡œ ê²¬ê³ í•œ ì•™ìƒë¸” í›ˆë ¨ ì‹œì‘...")
        
        if CFG['ENABLE_OPTIMIZATION']:
            # Optuna ìµœì í™” ì‹¤í–‰
            optimized_params = self.optimize_models(X_train_full, y_train_full)
        else:
            # ì €ì¥ëœ íŒŒë¼ë¯¸í„° ë¡œë“œ
            try:
                with open('optimized_params.pkl', 'rb') as f:
                    optimized_params = pickle.load(f)
                print("ğŸ’¾ ì €ì¥ëœ ìµœì í™” íŒŒë¼ë¯¸í„°ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            except FileNotFoundError:
                print("âŒ ì €ì¥ëœ íŒŒë¼ë¯¸í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ìµœì í™”ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.")
                optimized_params = self.optimize_models(X_train_full, y_train_full)
        
        # ğŸ›¡ï¸ ìµœì í™”ëœ íŒŒë¼ë¯¸í„°ë¡œ ê²¬ê³ í•œ ì•™ìƒë¸” í›ˆë ¨
        all_predictions = []
        oof_predictions = np.zeros(len(X_train_full))
        
        for seed in CFG['SEEDS']:
            print(f"\nğŸ”„ ì‹œë“œ {seed} ì•™ìƒë¸” í›ˆë ¨ ì¤‘...")
            seed_everything(seed)
            
            seed_test_preds = []
            
            # ë‹¤ì–‘í•œ CV ì „ëµ
            cv_strategies = [
                ('KFold', KFold(n_splits=15, shuffle=True, random_state=seed)),
                ('RepeatedKFold', RepeatedKFold(n_splits=8, n_repeats=2, random_state=seed)),
                ('ShuffleSplit', ShuffleSplit(n_splits=10, test_size=0.25, random_state=seed))
            ]
            
            for cv_name, cv_splitter in cv_strategies:
                fold_count = 0
                for train_idx, val_idx in cv_splitter.split(X_train_full, y_train_full):
                    fold_count += 1
                    if fold_count > 3:  # ì‹œê°„ ì ˆì•½
                        break
                        
                    X_train, X_val = X_train_full[train_idx], X_train_full[val_idx]
                    y_train, y_val = y_train_full.iloc[train_idx], y_train_full.iloc[val_idx]
                    
                    scaler = RobustScaler()
                    X_train_scaled = scaler.fit_transform(X_train)
                    X_val_scaled = scaler.transform(X_val)
                    X_test_scaled = scaler.transform(X_test_full)
                    
                    # ìµœì í™”ëœ ëª¨ë¸ë“¤ í›ˆë ¨
                    models = [
                        ('lgb', lgb.LGBMRegressor(**{**{'objective': 'regression', 'metric': 'rmse', 
                                                      'verbose': -1, 'n_jobs': -1}, 
                                                   **optimized_params['lgb']})),
                        ('xgb', xgb.XGBRegressor(**optimized_params['xgb'])),
                        ('rf', RandomForestRegressor(**optimized_params['rf'])),
                        ('et', ExtraTreesRegressor(n_estimators=500, max_depth=8, random_state=seed, n_jobs=-1))
                    ]
                    
                    fold_test_preds = []
                    
                    for model_name, model in models:
                        try:
                            if model_name == 'lgb':
                                model.fit(X_train_scaled, y_train, eval_set=[(X_val_scaled, y_val)],
                                         callbacks=[lgb.early_stopping(100, verbose=False)])
                            elif model_name == 'xgb':
                                try:
                                    model.fit(X_train_scaled, y_train, eval_set=[(X_val_scaled, y_val)],
                                             callbacks=[xgb.callback.EarlyStopping(rounds=100)])
                                except:
                                    model.fit(X_train_scaled, y_train)
                            else:
                                model.fit(X_train_scaled, y_train)
                            
                            val_pred = model.predict(X_val_scaled)
                            test_pred = model.predict(X_test_scaled)
                            
                            # OOF ì˜ˆì¸¡ ëˆ„ì 
                            oof_predictions[val_idx] += val_pred / (len(CFG['SEEDS']) * len(cv_strategies) * 3 * len(models))
                            fold_test_preds.append(test_pred)
                            
                        except Exception as e:
                            print(f"    {model_name} ì‹¤íŒ¨: {e}")
                            continue
                    
                    if fold_test_preds:
                        seed_test_preds.append(np.mean(fold_test_preds, axis=0))
            
            if seed_test_preds:
                all_predictions.append(np.mean(seed_test_preds, axis=0))
        
        # ìµœì¢… ì•™ìƒë¸”
        if all_predictions:
            final_test_preds = np.mean(all_predictions, axis=0)
        else:
            final_test_preds = np.full(len(X_test_full), y_train_full.mean())
        
        # ì„±ëŠ¥ í‰ê°€
        final_score, A, B, corr = self.get_leaderboard_score(y_train_full, oof_predictions)
        
        # ë¬´ì‘ìœ„ 80% ìƒ˜í”Œë§ ì‹œë®¬ë ˆì´ì…˜
        random_scores = []
        for _ in range(1000):
            sample_size = int(len(y_train_full) * 0.8)
            random_indices = np.random.choice(len(y_train_full), size=sample_size, replace=False)
            score, _, _, _ = self.get_leaderboard_score(y_train_full.iloc[random_indices], 
                                                       oof_predictions[random_indices])
            random_scores.append(score)
        
        print(f"\nğŸ† ìµœì¢… ìµœì í™”ëœ ì•™ìƒë¸” ì„±ëŠ¥:")
        print(f"ì „ì²´ ë°ì´í„° ìŠ¤ì½”ì–´: {final_score:.4f}")
        print(f"ë¬´ì‘ìœ„ 80% í‰ê· : {np.mean(random_scores):.4f} Â± {np.std(random_scores):.4f}")
        print(f"ë¬´ì‘ìœ„ 80% ë²”ìœ„: {np.min(random_scores):.4f} ~ {np.max(random_scores):.4f}")
        print(f"ìƒê´€ê´€ê³„ (B): {B:.4f}")
        
        # í›„ì²˜ë¦¬
        final_test_preds = np.clip(final_test_preds, 0, 100)
        
        return final_test_preds

def main():
    print("ğŸ”¥ Optuna ìµœì í™” + ë¬´ì‘ìœ„ 80% ìƒ˜í”Œë§ ê²¬ê³  ëª¨ë¸ ğŸ”¥")
    print("=" * 80)
    print(f"ğŸ¯ Optuna ìµœì í™”: {'í™œì„±í™”' if CFG['ENABLE_OPTIMIZATION'] else 'ë¹„í™œì„±í™”'}")
    print(f"ğŸ”¥ ìµœì í™” ì‹œí–‰ ìˆ˜: {CFG['OPTIMIZATION_TRIALS']}")
    print(f"â° ìµœì í™” íƒ€ì„ì•„ì›ƒ: {CFG['OPTIMIZATION_TIMEOUT']//60}ë¶„")
    print(f"âš¡ ì˜ˆìƒ ì‹¤í–‰ ì‹œê°„: {'60-90ë¶„' if CFG['ENABLE_OPTIMIZATION'] else '20-30ë¶„'}")
    
    try:
        # ë°ì´í„° ë¡œë“œ
        print("\në°ì´í„° ë¡œë“œ ì¤‘...")
        train_df = pd.read_csv('data/train.csv')
        test_df = pd.read_csv('data/test.csv')
        sample_submission = pd.read_csv('data/sample_submission.csv')
        
        print(f"í›ˆë ¨ ë°ì´í„°: {len(train_df)}ê°œ")
        print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_df)}ê°œ")
        print(f"0 ë¼ë²¨ ê°œìˆ˜: {(train_df['Inhibition'] == 0).sum()}ê°œ (ëª¨ë‘ ìœ ì§€)")
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        predictor = OptimizedRobustPredictor()
        
        # ê²¬ê³ í•œ íŠ¹ì„± ì¶”ì¶œ
        X_train_full = predictor.prepare_robust_data(train_df, is_training=True)
        X_test_full = predictor.prepare_robust_data(test_df, is_training=False)
        y_train_full = train_df['Inhibition']
        
        print(f"\nğŸš€ ê²¬ê³ í•œ íŠ¹ì„± ìˆ˜: {X_train_full.shape[1]:,}")
        print(f"ğŸ›¡ï¸ ì‹œë“œ ê°œìˆ˜: {len(CFG['SEEDS'])}ê°œ")
        
        # ìµœì í™”ëœ ì•™ìƒë¸” í›ˆë ¨
        test_preds = predictor.train_optimized_ensemble(X_train_full, y_train_full, X_test_full)
        
        # ì œì¶œ íŒŒì¼ ìƒì„±
        submission = sample_submission.copy()
        submission['Inhibition'] = test_preds
        submission['Inhibition'] = np.clip(submission['Inhibition'], 0, 100)
        
        submission.to_csv('submission_optimized_robust.csv', index=False)
        print(f"\nâœ… ì œì¶œ íŒŒì¼ì´ 'submission_optimized_robust.csv'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        print("\nğŸ¯ ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ ìš”ì•½:")
        print(submission['Inhibition'].describe())
        print(f"\nğŸ† ëª©í‘œ: Optuna ìµœì í™”ë¡œ ë¬´ì‘ìœ„ 80% ìƒ˜í”Œë§ì— ê²¬ê³ í•œ ìµœê³  ì„±ëŠ¥!")

    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 