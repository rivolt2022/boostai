import pandas as pd
import numpy as np
import os
import random
from rdkit import Chem
from rdkit.Chem import AllChem, DataStructs, Descriptors, rdMolDescriptors
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.model_selection import KFold, StratifiedShuffleSplit
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import lightgbm as lgb
import optuna
import matplotlib.pyplot as plt
import seaborn as sns
import warnings

warnings.filterwarnings('ignore')

# ì „ì—­ ì„¤ì • ë³€ìˆ˜ë“¤
CFG = {
    'NBITS': 2048,      # Morgan ì§€ë¬¸ì˜ ë¹„íŠ¸ ìˆ˜
    'SEED': 42,         # ì¬í˜„ì„±ì„ ìœ„í•œ ëœë¤ ì‹œë“œ
    'N_SPLITS': 10,     # K-í´ë“œ êµì°¨ ê²€ì¦ì—ì„œ ì‚¬ìš©í•  í´ë“œ ìˆ˜ (ëŠ˜ë ¤ì„œ ì•ˆì •ì„± í™•ë³´)
    'N_TRIALS': 50      # Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œë„ íšŸìˆ˜ (íƒìƒ‰ ê¸°íšŒ ì¦ê°€)
}

def seed_everything(seed):
    """ëª¨ë“  ëœë¤ ì‹œë“œë¥¼ ì„¤ì •í•˜ì—¬ ì‹¤í—˜ì˜ ì¬í˜„ì„±ì„ ë³´ì¥í•˜ëŠ” í•¨ìˆ˜"""
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)

# ì „ì—­ ì‹œë“œ ì„¤ì •
seed_everything(CFG['SEED'])

class CYP3A4InhibitionPredictor:
    def __init__(self):
        self.model = None
        self.scaler = RobustScaler() # ì´ìƒì¹˜ì— ê°•í•œ RobustScaler ìœ ì§€
        self.feature_names = None
        self.best_params = None
        # RDKitì—ì„œ ê³„ì‚° ê°€ëŠ¥í•œ ëª¨ë“  ì„¤ëª…ì ë¦¬ìŠ¤íŠ¸
        self.descriptor_names = [desc_name for desc_name, _ in Descriptors._descList]

    def get_all_descriptors(self, mol):
        """RDKitì˜ ëª¨ë“  ë¶„ì ì„¤ëª…ìë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜"""
        desc_dict = {}
        for name in self.descriptor_names:
            try:
                desc_func = getattr(Descriptors, name)
                desc_dict[name] = desc_func(mol)
            except:
                # ê³„ì‚° ì‹¤íŒ¨ ì‹œ 0ìœ¼ë¡œ ì±„ì›€
                desc_dict[name] = 0
        return desc_dict

    def smiles_to_features(self, smiles):
        """SMILES ë¬¸ìì—´ì—ì„œ Morgan ì§€ë¬¸, MACCS í‚¤, ë¶„ì ì„¤ëª…ìë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜"""
        mol = Chem.MolFromSmiles(smiles)
        if mol is None:
            return None, None, None

        # 1. Morgan Fingerprint
        fp_morgan = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=CFG['NBITS'])
        arr_morgan = np.zeros((1,))
        DataStructs.ConvertToNumpyArray(fp_morgan, arr_morgan)

        # 2. MACCS Keys
        fp_maccs = AllChem.GetMACCSKeysFingerprint(mol)
        arr_maccs = np.zeros((1,))
        DataStructs.ConvertToNumpyArray(fp_maccs, arr_maccs)

        # 3. RDKit Descriptors
        descriptors = self.get_all_descriptors(mol)

        return arr_morgan, arr_maccs, descriptors

    def prepare_data(self, df):
        """ë°ì´í„°í”„ë ˆì„ì—ì„œ ëª¨ë“  íŠ¹ì„±ì„ ë³‘ë ¬ë¡œ ì¶”ì¶œ"""
        print("ë¶„ì íŠ¹ì„± ì¶”ì¶œ ì¤‘ (Morgan, MACCS, RDKit Descriptors)...")
        
        all_features = []
        for i, smiles in enumerate(df['Canonical_Smiles']):
            if i % 200 == 0:
                print(f"ì²˜ë¦¬ ì¤‘: {i}/{len(df)}")
            
            morgan_fp, maccs_fp, descriptors = self.smiles_to_features(smiles)
            
            if morgan_fp is None:
                # SMILES íŒŒì‹± ì‹¤íŒ¨ ì‹œ
                morgan_fp = np.zeros(CFG['NBITS'])
                maccs_fp = np.zeros(167) # MACCS í‚¤ëŠ” 167 ë¹„íŠ¸
                descriptors = {name: 0 for name in self.descriptor_names}

            all_features.append((morgan_fp, maccs_fp, list(descriptors.values())))

        # íŠ¹ì„±ë³„ë¡œ ë¶„ë¦¬
        morgan_fps = np.array([item[0] for item in all_features])
        maccs_fps = np.array([item[1] for item in all_features])
        desc_df = pd.DataFrame([item[2] for item in all_features], columns=self.descriptor_names)

        # íŠ¹ì„± ì´ë¦„ ì €ì¥ (ë‚˜ì¤‘ì— íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™”ë¥¼ ìœ„í•´)
        morgan_names = [f'Morgan_{i}' for i in range(CFG['NBITS'])]
        maccs_names = [f'MACCS_{i}' for i in range(maccs_fps.shape[1])]
        self.feature_names = morgan_names + maccs_names + self.descriptor_names
        
        # ëª¨ë“  íŠ¹ì„±ì„ í•˜ë‚˜ì˜ numpy ë°°ì—´ë¡œ ê²°í•©
        return np.hstack([morgan_fps, maccs_fps, desc_df.values])

    def get_score(self, y_true, y_pred):
        """ì»¤ìŠ¤í…€ ìŠ¤ì½”ì–´ í•¨ìˆ˜"""
        mse = mean_squared_error(y_true, y_pred)
        rmse = np.sqrt(mse)
        # y_trueì˜ ë²”ìœ„ê°€ 0~100ìœ¼ë¡œ ê³ ì •ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ë¶„ëª¨ë¥¼ ìƒìˆ˜ë¡œ ì‚¬ìš© ê°€ëŠ¥
        nrmse = rmse / (100 - 0) 
        A = 1 - min(nrmse, 1)
        B = r2_score(y_true, y_pred)
        score = 0.4 * A + 0.6 * B
        return score

    def objective(self, trial, X, y):
        """Optuna ëª©ì  í•¨ìˆ˜"""
        params = {
            'objective': 'regression_l1', # MAE objective, ì´ìƒì¹˜ì— ë” ê°•í•¨
            'metric': 'rmse',
            'verbose': -1,
            'n_jobs': -1,
            'seed': CFG['SEED'],
            'boosting_type': 'gbdt',
            'n_estimators': trial.suggest_int('n_estimators', 500, 2000),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.05),
            'num_leaves': trial.suggest_int('num_leaves', 20, 60),
            'max_depth': trial.suggest_int('max_depth', 5, 12),
            'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 0.9),
            'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 0.9),
            'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),
            'min_child_samples': trial.suggest_int('min_child_samples', 5, 30),
            'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),
            'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),
        }

        kf = KFold(n_splits=CFG['N_SPLITS'], shuffle=True, random_state=CFG['SEED'])
        oof_preds = np.zeros(len(X))
        y_array = y.values if hasattr(y, 'values') else np.array(y)

        for train_idx, val_idx in kf.split(X, y_array):
            X_train, X_val = X[train_idx], X[val_idx]
            y_train, y_val = y_array[train_idx], y_array[val_idx]

            # ë°ì´í„° ìŠ¤ì¼€ì¼ë§
            scaler = RobustScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_val_scaled = scaler.transform(X_val)

            model = lgb.LGBMRegressor(**params)
            model.fit(X_train_scaled, y_train, eval_set=[(X_val_scaled, y_val)],
                      eval_metric='rmse', callbacks=[lgb.early_stopping(100, verbose=False)])
            
            oof_preds[val_idx] = model.predict(X_val_scaled)

        score = self.get_score(y_array, oof_preds)
        return score

    def train_and_predict(self, X_train_full, y_train_full, X_test_full):
        """ëª¨ë¸ í›ˆë ¨ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡"""
        print("í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì¤‘...")
        study = optuna.create_study(direction='maximize', study_name='lgbm_tuning')
        study.optimize(lambda trial: self.objective(trial, X_train_full, y_train_full), n_trials=CFG['N_TRIALS'])

        print(f"ìµœì í™” ì™„ë£Œ. ìµœê³  ìŠ¤ì½”ì–´: {study.best_value:.4f}")
        self.best_params = study.best_params
        print("ìµœì  íŒŒë¼ë¯¸í„°:", self.best_params)

        # K-Fold ì•™ìƒë¸” í›ˆë ¨ ë° ì˜ˆì¸¡
        print("\nì•™ìƒë¸” ì˜ˆì¸¡ì„ ìœ„í•œ K-í´ë“œ í›ˆë ¨ ì¤‘...")
        kf = KFold(n_splits=CFG['N_SPLITS'], shuffle=True, random_state=CFG['SEED'])
        test_preds = np.zeros(len(X_test_full))
        oof_preds = np.zeros(len(X_train_full))
        
        final_model_params = {
            'objective': 'regression_l1',
            'metric': 'rmse', 'verbose': -1, 'n_jobs': -1,
            'seed': CFG['SEED'], 'boosting_type': 'gbdt'
        }
        final_model_params.update(self.best_params)

        models = [] # íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™”ë¥¼ ìœ„í•´ ëª¨ë¸ ì €ì¥

        for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_full, y_train_full)):
            print(f"--- í›ˆë ¨ í´ë“œ {fold+1}/{CFG['N_SPLITS']} ---")
            X_train, X_val = X_train_full[train_idx], X_train_full[val_idx]
            y_train, y_val = y_train_full.iloc[train_idx], y_train_full.iloc[val_idx]

            # â— ì¤‘ìš”: í´ë“œë§ˆë‹¤ ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ ìƒˆë¡œ fit_transform í•´ì•¼ ë°ì´í„° ëˆ„ìˆ˜ë¥¼ ë§‰ì„ ìˆ˜ ìˆìŒ
            scaler = RobustScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_val_scaled = scaler.transform(X_val)
            X_test_scaled = scaler.transform(X_test_full)

            model = lgb.LGBMRegressor(**final_model_params)
            model.fit(X_train_scaled, y_train, eval_set=[(X_val_scaled, y_val)],
                      eval_metric='rmse', callbacks=[lgb.early_stopping(100, verbose=False)])

            oof_preds[val_idx] = model.predict(X_val_scaled)
            test_preds += model.predict(X_test_scaled) / CFG['N_SPLITS']
            models.append(model)
        
        self.model = models # ë§ˆì§€ë§‰ í´ë“œì˜ ëª¨ë¸ì„ ëŒ€í‘œë¡œ ì €ì¥ (í˜¹ì€ í‰ê·  ëª¨ë¸)
        
        # OOF ì˜ˆì¸¡ ê²°ê³¼ë¡œ ì „ì²´ í›ˆë ¨ ë°ì´í„°ì— ëŒ€í•œ ì„±ëŠ¥ í‰ê°€
        final_score = self.get_score(y_train_full, oof_preds)
        print(f"\nK-Fold OOF Custom Score: {final_score:.4f}")
        
        self.plot_results(y_train_full, oof_preds, "K-Fold OOF Predictions")

        return test_preds

    def plot_results(self, y_true, y_pred, title="ì˜ˆì¸¡ vs ì‹¤ì œ"):
        """ê²°ê³¼ ì‹œê°í™”"""
        plt.figure(figsize=(14, 8))
        plt.rcParams['font.family'] = 'Malgun Gothic' # í•œê¸€ í°íŠ¸ ì„¤ì •
        plt.rcParams['axes.unicode_minus'] = False # ë§ˆì´ë„ˆìŠ¤ ê¹¨ì§ ë°©ì§€
        
        # ì‚°ì ë„
        plt.subplot(2, 2, 1)
        sns.regplot(x=y_true, y=y_pred, scatter_kws={'alpha':0.4, 'color': 'royalblue'}, line_kws={'color':'red', 'linestyle':'--'})
        plt.xlabel('ì‹¤ì œ ê°’ (Inhibition %)')
        plt.ylabel('ì˜ˆì¸¡ ê°’ (Inhibition %)')
        plt.title(f'{title}\nRÂ² = {r2_score(y_true, y_pred):.4f}')
        
        # ì”ì°¨ í”Œë¡¯
        plt.subplot(2, 2, 2)
        residuals = y_true - y_pred
        sns.scatterplot(x=y_pred, y=residuals, alpha=0.5, color='forestgreen')
        plt.axhline(y=0, color='r', linestyle='--')
        plt.xlabel('ì˜ˆì¸¡ ê°’')
        plt.ylabel('ì”ì°¨')
        plt.title('ì”ì°¨ í”Œë¡¯')
        
        # íŠ¹ì„± ì¤‘ìš”ë„
        plt.subplot(2, 1, 2)
        if self.model and self.feature_names:
            # ëª¨ë“  í´ë“œì˜ íŠ¹ì„± ì¤‘ìš”ë„ë¥¼ í‰ê· ë‚´ì–´ ì‚¬ìš©
            importances = np.mean([m.feature_importances_ for m in self.model], axis=0)
            feature_importance_df = pd.DataFrame({
                'feature': self.feature_names,
                'importance': importances
            }).sort_values('importance', ascending=False).head(20)
            
            sns.barplot(x='importance', y='feature', data=feature_importance_df)
            plt.title('ìƒìœ„ 20ê°œ íŠ¹ì„± ì¤‘ìš”ë„ (K-Fold í‰ê· )')
        
        plt.tight_layout()
        plt.savefig('model_results_improved.png', dpi=300, bbox_inches='tight')
        plt.show()

def main():
    print("ğŸš€ CYP3A4 íš¨ì†Œ ì €í•´ ì˜ˆì¸¡ ëª¨ë¸ ê°œì„  ì‹œì‘ ğŸš€")
    print("=" * 70)
    
    try:
        # ë°ì´í„° ë¡œë“œ
        print("ë°ì´í„° ë¡œë“œ ì¤‘...")
        train_df = pd.read_csv('data/train.csv')
        test_df = pd.read_csv('data/test.csv')
        sample_submission = pd.read_csv('data/sample_submission.csv')
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        predictor = CYP3A4InhibitionPredictor()
        
        # íŠ¹ì„± ì¶”ì¶œ
        X_train_full = predictor.prepare_data(train_df)
        X_test_full = predictor.prepare_data(test_df)
        y_train_full = train_df['Inhibition']
        
        print(f"\nìµœì¢… íŠ¹ì„± ìˆ˜: {X_train_full.shape[1]}")
        
        # ëª¨ë¸ í›ˆë ¨ ë° ì˜ˆì¸¡
        test_preds = predictor.train_and_predict(X_train_full, y_train_full, X_test_full)
        
        # ì œì¶œ íŒŒì¼ ìƒì„±
        submission = sample_submission.copy()
        submission['Inhibition'] = test_preds
        submission['Inhibition'] = np.clip(submission['Inhibition'], 0, 100)
        
        submission.to_csv('submission_improved.csv', index=False)
        print(f"\nâœ… ì œì¶œ íŒŒì¼ì´ 'submission_improved.csv'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        print("\nì˜ˆì¸¡ ê²°ê³¼ ìš”ì•½:")
        print(submission['Inhibition'].describe())

    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()