# --- ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ---
# ë°ì´í„° ë¶„ì„ ë° ì²˜ë¦¬ë¥¼ ìœ„í•œ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬
import pandas as pd  # ë°ì´í„°í”„ë ˆì„(í‘œ í˜•íƒœì˜ ë°ì´í„°)ì„ ë‹¤ë£¨ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import numpy as np   # ìˆ˜ì¹˜ ê³„ì‚°, íŠ¹íˆ ë°°ì—´(í–‰ë ¬) ì—°ì‚°ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬

# íŒŒì´ì¬ ê¸°ë³¸ ë‚´ì¥ ë¼ì´ë¸ŒëŸ¬ë¦¬
import os            # ìš´ì˜ì²´ì œì™€ ìƒí˜¸ì‘ìš©í•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ (ì˜ˆ: í™˜ê²½ ë³€ìˆ˜ ì„¤ì •)
import random        # ë¬´ì‘ìœ„ ìˆ˜ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from pathlib import Path  # íŒŒì¼ ë° ë””ë ‰í† ë¦¬ ê²½ë¡œë¥¼ ê°ì²´ ì§€í–¥ì ìœ¼ë¡œ ë‹¤ë£¨ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬

# í™”í•™ ì •ë³´í•™ ë° íŠ¹ì§• ê³µí•™ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from rdkit import Chem  # ë¶„ì êµ¬ì¡°ë¥¼ ë‹¤ë£¨ê³  í™”í•™ ê³„ì‚°ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
from rdkit.Chem import AllChem, DataStructs, Descriptors  # Morgan Fingerprint, ë¶„ì ì„¤ëª…ì ë“± ê³„ì‚° ê¸°ëŠ¥

# ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë§ ë° í‰ê°€ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from sklearn.preprocessing import StandardScaler   # ë°ì´í„°ì˜ ìŠ¤ì¼€ì¼ì„ ì¡°ì •(ì •ê·œí™”)í•˜ê¸° ìœ„í•œ ë„êµ¬
from sklearn.model_selection import KFold          # êµì°¨ ê²€ì¦ì„ ìœ„í•´ ë°ì´í„°ë¥¼ ì—¬ëŸ¬ ë¶€ë¶„ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ë„êµ¬
from sklearn.metrics import mean_squared_error   # ëª¨ë¸ì˜ ì˜ˆì¸¡ ì˜¤ì°¨(MSE)ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜
from scipy.stats import pearsonr                 # ë‘ ë³€ìˆ˜ ê°„ì˜ í”¼ì–´ìŠ¨ ìƒê´€ ê³„ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜
import lightgbm as lgb                           # ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸
import optuna                                    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ë¥¼ ìë™í™”í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬

# --- ì‹ ê²½ë§ ëª¨ë¸ ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€ ---
import torch
from transformers import AutoTokenizer, AutoModel

# --- ì „ì—­ ì„¤ì • (Global Configuration) ---
# ì‹¤í—˜ì˜ ì£¼ìš” íŒŒë¼ë¯¸í„°ë“¤ì„ ì½”ë“œ ìƒë‹¨ì— ëª¨ì•„ë‘ì–´ ê´€ë¦¬í•˜ê¸° ì‰½ê²Œ í•¨
CFG = {
    'NBITS': 2048,      # Morgan Fingerprintë¥¼ ìƒì„±í•  ë•Œ ì‚¬ìš©í•  ë¹„íŠ¸(ì°¨ì›)ì˜ ìˆ˜
    'FP_RADIUS': 3,     # Morgan Fingerprint ê³„ì‚° ì‹œ ê³ ë ¤í•  ì›ìì˜ ë°˜ê²½. í´ìˆ˜ë¡ ë” ë„“ì€ êµ¬ì¡° ì •ë³´ë¥¼ í¬í•¨.
    'SEEDS': [42, 2024, 101, 7, 99], # ì‹œë“œ ì•™ìƒë¸”ì— ì‚¬ìš©í•  ì—¬ëŸ¬ ê°œì˜ ëœë¤ ì‹œë“œ ëª©ë¡
    'N_SPLITS': 10,     # K-Fold êµì°¨ ê²€ì¦ ì‹œ ë°ì´í„°ë¥¼ ë‚˜ëˆŒ í´ë“œ(Fold)ì˜ ìˆ˜
    'N_TRIALS': 50,     # Optunaê°€ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ë¥¼ ìœ„í•´ ì‹œë„í•  íšŸìˆ˜ (ì‹œê°„ ê´€ê³„ìƒ ì¶•ì†Œ)
    'CHEMBERTA_MODEL': 'seyonec/ChemBERTa-zinc-base-v1' # ì‚¬ìš©í•  ì‚¬ì „ í›ˆë ¨ ëª¨ë¸
}

# --- í•¨ìˆ˜ ì •ì˜ ---

def seed_everything(seed):
    """
    ì¬í˜„ì„±ì„ ìœ„í•´ ëª¨ë“  ì¢…ë¥˜ì˜ ëœë¤ ì‹œë“œë¥¼ ê³ ì •í•˜ëŠ” í•¨ìˆ˜.
    ì´ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ë©´ ì½”ë“œë¥¼ ì—¬ëŸ¬ ë²ˆ ì‹¤í–‰í•´ë„ í•­ìƒ ë™ì¼í•œ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŒ.
    """
    random.seed(seed)  # íŒŒì´ì¬ ë‚´ì¥ random ëª¨ë“ˆì˜ ì‹œë“œ ê³ ì •
    os.environ['PYTHONHASHSEED'] = str(seed)  # íŒŒì´ì¬ í•´ì‹œ í•¨ìˆ˜ì˜ ì‹œë“œ ê³ ì •
    np.random.seed(seed)  # NumPy ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ì‹œë“œ ê³ ì •

# Optuna íŠœë‹ ë° ì´ˆê¸° ë°ì´í„° ë¶„í• ì˜ ì¼ê´€ì„±ì„ ìœ„í•´ ì²« ë²ˆì§¸ ì‹œë“œë¡œ ì´ˆê¸°í™”
seed_everything(CFG['SEEDS'][0])

def load_data():
    """
    ëŒ€íšŒì—ì„œ ì œê³µëœ 'train.csv'ì™€ 'test.csv' ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
    """
    try:
        data_dir = Path("./data")  # ë°ì´í„° íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ
        train_df = pd.read_csv(data_dir / "train.csv") # í›ˆë ¨ ë°ì´í„° ë¡œë“œ
        test_df = pd.read_csv(data_dir / "test.csv")   # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
        return train_df, test_df
    except FileNotFoundError as e:
        # íŒŒì¼ì´ ì—†ì„ ê²½ìš° ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ì¶œë ¥í•˜ê³  í”„ë¡œê·¸ë¨ì„ ì•ˆì „í•˜ê²Œ ì¢…ë£Œ
        print(f"ì˜¤ë¥˜: {e}. 'data' ë””ë ‰í† ë¦¬ì— íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        return None, None

def get_chemberta_embeddings(smiles_list, model_name, batch_size=32):
    """
    SMILES ë¦¬ìŠ¤íŠ¸ë¡œë¶€í„° ì‚¬ì „ í›ˆë ¨ëœ ChemBERTa ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì„ë² ë”©ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜.
    """
    print(f"'{model_name}' ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì„ë² ë”© ì¶”ì¶œ ì¤‘...")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModel.from_pretrained(model_name)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval()

    all_embeddings = []
    with torch.no_grad():
        for i in range(0, len(smiles_list), batch_size):
            batch_smiles = smiles_list[i:i+batch_size]
            inputs = tokenizer(batch_smiles, return_tensors="pt", padding=True, truncation=True, max_length=128).to(device)
            outputs = model(**inputs)
            # [CLS] í† í°ì˜ ì„ë² ë”©ì„ ì‚¬ìš© (ë¶„ì ì „ì²´ì˜ ëŒ€í‘œ ë²¡í„°)
            cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
            all_embeddings.extend(cls_embeddings)
            if (i // batch_size) % 10 == 0:
                print(f"  {i+len(batch_smiles)} / {len(smiles_list)} ì²˜ë¦¬ ì™„ë£Œ...")

    print("ì„ë² ë”© ì¶”ì¶œ ì™„ë£Œ.")
    return np.array(all_embeddings)

def smiles_to_fingerprint(smiles):
    """
    ë¶„ìì˜ êµ¬ì¡° ì •ë³´(SMILES ë¬¸ìì—´)ë¥¼ ìˆ«ì ë²¡í„°(Morgan Fingerprint)ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜.
    ëª¨ë¸ì´ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ìˆ«ì ì •ë³´ë¡œ ë°”ê¾¸ëŠ” ê³¼ì •.
    """
    mol = Chem.MolFromSmiles(smiles)  # SMILES ë¬¸ìì—´ì„ RDKit ë¶„ì ê°ì²´ë¡œ ë³€í™˜
    if mol is not None:
        # Morgan Fingerprint ìƒì„±. ë¶„ì ë‚´ ê° ì›ì ì£¼ë³€ì˜ êµ¬ì¡°ì  íŠ¹ì§•ì„ ìš”ì•½í•œ ê²ƒ.
        fp = AllChem.GetMorganFingerprintAsBitVect(mol, CFG['FP_RADIUS'], nBits=CFG['NBITS'])
        arr = np.zeros((1,))  # ê²°ê³¼ë¥¼ ë‹´ì„ NumPy ë°°ì—´ ì´ˆê¸°í™”
        DataStructs.ConvertToNumpyArray(fp, arr)  # Fingerprintë¥¼ NumPy ë°°ì—´ë¡œ ë³€í™˜
        return arr
    return None  # ë¶„ì ê°ì²´ ìƒì„± ì‹¤íŒ¨ ì‹œ None ë°˜í™˜

def calculate_rdkit_descriptors(smiles):
    """
    SMILES ë¬¸ìì—´ë¡œë¶€í„° ì•½ 200ì—¬ ê°œì˜ ë¬¼ë¦¬í™”í•™ì  íŠ¹ì„±(ë¶„ì ì„¤ëª…ì)ì„ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜.
    ì˜ˆ: ë¶„ìëŸ‰(MolWt), ë¡œê·¸ P(MolLogP) ë“±.
    """
    mol = Chem.MolFromSmiles(smiles)
    if mol is None:
        # ë¶„ì ê°ì²´ ìƒì„± ì‹¤íŒ¨ ì‹œ, ëª¨ë“  ì„¤ëª…ì ê°’ì„ NaN(Not a Number)ìœ¼ë¡œ ì±„ìš´ ë°°ì—´ ë°˜í™˜
        return np.full((len(Descriptors._descList),), np.nan)
    # RDKitì—ì„œ ì œê³µí•˜ëŠ” ëª¨ë“  ì„¤ëª…ì í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ê°’ì„ ê³„ì‚°
    descriptors = [desc_func(mol) for _, desc_func in Descriptors._descList]
    return np.array(descriptors)

def get_score(y_true, y_pred):
    """
    ëŒ€íšŒ í‰ê°€ ì‚°ì‹ì— ë”°ë¼ ëª¨ë¸ì˜ ì„±ëŠ¥ ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜.
    Score = 0.5 * (1 - min(A, 1)) + 0.5 * B
    A = Normalized RMSE (ì •ê·œí™”ëœ í‰ê·  ì œê³±ê·¼ ì˜¤ì°¨)
    B = Pearson Correlation Coefficient (í”¼ì–´ìŠ¨ ìƒê´€ ê³„ìˆ˜)
    """
    # --- A ê³„ì‚°: Normalized RMSE (NRMSE) ---
    # ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ ì°¨ì´(ì˜¤ì°¨)ë¥¼ ì¸¡ì •í•˜ëŠ” ì§€í‘œ. ì‘ì„ìˆ˜ë¡ ì¢‹ìŒ.
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    # ì‹¤ì œê°’ì˜ ë²”ìœ„ (ìµœëŒ€ê°’ - ìµœì†Œê°’)
    y_true_range = np.max(y_true) - np.min(y_true)
    # ë¶„ëª¨ê°€ 0ì´ ë˜ëŠ” ê·¹ë‹¨ì ì¸ ê²½ìš°ë¥¼ ë°©ì§€
    if y_true_range == 0:
        nrmse = 0 if rmse == 0 else np.inf
    else:
        # RMSEë¥¼ ì‹¤ì œê°’ì˜ ë²”ìœ„ë¡œ ë‚˜ëˆ„ì–´ ìŠ¤ì¼€ì¼ì— ë¬´ê´€í•˜ê²Œ ë§Œë“¤ì–´ ì¤Œ
        nrmse = rmse / y_true_range
    A = nrmse
    
    # --- B ê³„ì‚°: Pearson Correlation Coefficient ---
    # ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ ì‚¬ì´ì˜ 'ì„ í˜• ê´€ê³„'ì˜ ê°•ë„ë¥¼ ì¸¡ì •. 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ.
    # ì¦‰, ì˜ˆì¸¡ê°’ì´ ì‹¤ì œê°’ì˜ ë³€í™” ê²½í–¥ì„±(ì˜¤ë¥´ë‚´ë¦¼)ì„ ì–¼ë§ˆë‚˜ ì˜ ë”°ë¼ê°€ëŠ”ì§€ë¥¼ ë‚˜íƒ€ëƒ„.
    if np.std(y_true) < 1e-6 or np.std(y_pred) < 1e-6:
        # ë°ì´í„°ì˜ ëª¨ë“  ê°’ì´ ê±°ì˜ ë™ì¼í•˜ì—¬ ë¶„ì‚°ì´ 0ì— ê°€ê¹Œìš°ë©´ ìƒê´€ê³„ìˆ˜ ê³„ì‚°ì´ ë¶ˆê°€ëŠ¥
        correlation = 0.0
    else:
        correlation, _ = pearsonr(y_true, y_pred)
    # í‰ê°€ ì‚°ì‹ì— ë”°ë¼ ìƒê´€ê³„ìˆ˜ ê°’ì„ 0ê³¼ 1 ì‚¬ì´ë¡œ ì œí•œ(clip)
    B = np.clip(correlation, 0, 1)

    # ìµœì¢… ì ìˆ˜ ê³„ì‚° (ë‘ ì§€í‘œë¥¼ 0.5ì”© ê°€ì¤‘ í‰ê· )
    score = 0.5 * (1 - min(A, 1)) + 0.5 * B
    return score

def lgbm_score_metric(y_true, y_pred):
    """
    LightGBM ëª¨ë¸ í›ˆë ¨ ì‹œ, ì¡°ê¸° ì¢…ë£Œ(Early Stopping) ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ì»¤ìŠ¤í…€ í‰ê°€ì§€í‘œ í•¨ìˆ˜.
    """
    score = get_score(y_true, y_pred)
    # LightGBMì´ ì¸ì‹í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë°˜í™˜: (í‰ê°€ì§€í‘œ ì´ë¦„, ì ìˆ˜, ë†’ì€ ì ìˆ˜ê°€ ì¢‹ì€ì§€ ì—¬ë¶€)
    return 'custom_score', score, True # is_higher_better=True. Trueì´ë¯€ë¡œ ì ìˆ˜ê°€ ë†’ì•„ì§€ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµ.

def objective(trial, X, y):
    """
    Optuna ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ìµœì í™”í•˜ê¸° ìœ„í•´ í˜¸ì¶œí•˜ëŠ” ëª©ì  í•¨ìˆ˜.
    ì´ í•¨ìˆ˜ì˜ ë°˜í™˜ê°’(score)ì„ ìµœëŒ€í™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ìµœì ì˜ íŒŒë¼ë¯¸í„° ì¡°í•©ì„ íƒìƒ‰.
    """
    # Optunaê°€ íƒìƒ‰í•  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë“¤ì˜ ì´ë¦„ê³¼ ë²”ìœ„ë¥¼ ì •ì˜
    params = {
        'objective': 'regression',          # ëª©í‘œ: íšŒê·€(ìˆ«ì ì˜ˆì¸¡)
        'metric': 'rmse',                   # ê¸°ë³¸ í‰ê°€ì§€í‘œ (ì‹¤ì œë¡œëŠ” ì»¤ìŠ¤í…€ ì§€í‘œë¡œ ë®ì–´ì“°ë¯€ë¡œ í° ì˜ë¯¸ ì—†ìŒ)
        'verbose': -1,                      # í›ˆë ¨ ê³¼ì •ì˜ ë¡œê·¸ë¥¼ ì¶œë ¥í•˜ì§€ ì•ŠìŒ
        'n_jobs': -1,                       # ì»´í“¨í„°ì˜ ëª¨ë“  CPU ì½”ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ ì†ë„ í–¥ìƒ
        'seed': CFG['SEEDS'][0],            # íŠœë‹ ê³¼ì •ì˜ ì¬í˜„ì„±ì„ ìœ„í•´ ì‹œë“œë¥¼ ê³ ì •
        'boosting_type': 'gbdt',            # ì „í†µì ì¸ ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… ê²°ì • íŠ¸ë¦¬ ë°©ì‹ ì‚¬ìš©
        'n_estimators': 2000,               # ì•™ìƒë¸”í•  íŠ¸ë¦¬ì˜ ìµœëŒ€ ê°œìˆ˜ (ì¡°ê¸° ì¢…ë£Œë¡œ ìµœì  ê°œìˆ˜ ìë™ íƒìƒ‰)
        
        # --- Optunaê°€ ê°’ì„ ì œì•ˆ(suggest)í•˜ì—¬ ìµœì í™”í•  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë“¤ ---
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True), # í•™ìŠµë¥ . ë„ˆë¬´ í¬ë©´ ìµœì ì ì„ ì§€ë‚˜ì¹˜ê³ , ì‘ìœ¼ë©´ í›ˆë ¨ì´ ëŠë¦¼.
        'num_leaves': trial.suggest_int('num_leaves', 20, 100),                     # í•˜ë‚˜ì˜ íŠ¸ë¦¬ê°€ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ìµœëŒ€ ë¦¬í”„(í„°ë¯¸ë„) ë…¸ë“œì˜ ìˆ˜. ëª¨ë¸ì˜ ë³µì¡ë„ì™€ ê´€ë ¨.
        'max_depth': trial.suggest_int('max_depth', 3, 10),                         # íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´. ê³¼ì í•© ì œì–´.
        'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 1.0),      # ê° íŠ¸ë¦¬ë¥¼ í›ˆë ¨í•  ë•Œ ë¬´ì‘ìœ„ë¡œ ì„ íƒí•  íŠ¹ì§•(feature)ì˜ ë¹„ìœ¨.
        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 1.0),      # ê° íŠ¸ë¦¬ë¥¼ í›ˆë ¨í•  ë•Œ ë¬´ì‘ìœ„ë¡œ ì„ íƒí•  ë°ì´í„°(row)ì˜ ë¹„ìœ¨.
        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),                    # ëª‡ ë²ˆì˜ ì´í„°ë ˆì´ì…˜ë§ˆë‹¤ Baggingì„ ìˆ˜í–‰í• ì§€ ê²°ì •.
        'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),         # ë¦¬í”„ ë…¸ë“œê°€ ë˜ê¸° ìœ„í•´ í•„ìš”í•œ ìµœì†Œí•œì˜ ë°ì´í„° ìƒ˜í”Œ ìˆ˜. ê³¼ì í•© ì œì–´.
    }

    # êµì°¨ ê²€ì¦ì„ ìœ„í•œ ë°ì´í„° ë¶„í• ê¸° ì„¤ì • (Optuna íŠœë‹ ì‹œì—ëŠ” ê³ ì •ëœ ì‹œë“œ ì‚¬ìš©)
    kf = KFold(n_splits=CFG['N_SPLITS'], shuffle=True, random_state=CFG['SEEDS'][0])
    # Out-of-Fold (OOF) ì˜ˆì¸¡ê°’ì„ ì €ì¥í•˜ê¸° ìœ„í•œ ë°°ì—´ ì´ˆê¸°í™”.
    # OOF ì˜ˆì¸¡: ê° ë°ì´í„° í¬ì¸íŠ¸ê°€ 'ê²€ì¦ìš©'ìœ¼ë¡œ ì‚¬ìš©ë  ë•Œì˜ ì˜ˆì¸¡ê°’ì„ ëª¨ì€ ê²ƒ. ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ì¢‹ì€ ì²™ë„.
    oof_preds = np.zeros(len(X))

    # K-Fold êµì°¨ ê²€ì¦ ìˆ˜í–‰
    for train_idx, val_idx in kf.split(X, y):
        # í›ˆë ¨ ë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„° ë¶„í• 
        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
        y_train, y_val = y[train_idx], y[val_idx]
        
        # ì •ì˜ëœ íŒŒë¼ë¯¸í„°ë¡œ LightGBM ëª¨ë¸ ìƒì„±
        model = lgb.LGBMRegressor(**params)
        
        # ëª¨ë¸ í›ˆë ¨
        model.fit(X_train, y_train, 
                  eval_set=[(X_val, y_val)],           # í›ˆë ¨ ì¤‘ ì„±ëŠ¥ì„ ëª¨ë‹ˆí„°ë§í•  ê²€ì¦ ë°ì´í„°ì…‹ ì§€ì •
                  eval_metric=lgbm_score_metric,      # ì¡°ê¸° ì¢…ë£Œì˜ ê¸°ì¤€ìœ¼ë¡œ ì»¤ìŠ¤í…€ í‰ê°€ í•¨ìˆ˜ ì‚¬ìš©
                  callbacks=[lgb.early_stopping(100, verbose=False)]) # 100ë²ˆì˜ ì´í„°ë ˆì´ì…˜ ë™ì•ˆ ê²€ì¦ ì ìˆ˜ê°€ í–¥ìƒë˜ì§€ ì•Šìœ¼ë©´ í›ˆë ¨ì„ ì¡°ê¸° ì¢…ë£Œ
        
        # ê²€ì¦ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ ë° ê²°ê³¼ ì €ì¥
        # Inhibition ê°’ì€ 0~100 ì‚¬ì´ì˜ í¼ì„¼íŠ¸ ê°’ì´ë¯€ë¡œ, ì˜ˆì¸¡ê°’ë„ í•´ë‹¹ ë²”ìœ„ë¡œ í´ë¦¬í•‘í•˜ì—¬ ì•ˆì •ì„± í™•ë³´
        oof_preds[val_idx] = np.clip(model.predict(X_val), 0, 100)

    # ëª¨ë“  í´ë“œì˜ OOF ì˜ˆì¸¡ê°’ì„ ì‚¬ìš©í•˜ì—¬ ìµœì¢… ì„±ëŠ¥ ì ìˆ˜ ê³„ì‚°
    score = get_score(y, oof_preds)
    return score

# --- ë©”ì¸ ì‹¤í–‰ ë¸”ë¡ ---
# ì´ ìŠ¤í¬ë¦½íŠ¸ê°€ ì§ì ‘ ì‹¤í–‰ë  ë•Œë§Œ ì•„ë˜ ì½”ë“œê°€ ë™ì‘í•˜ë„ë¡ í•¨
if __name__ == "__main__":
    # === 1. ë°ì´í„° ë¡œë”© ===
    print("1. ë°ì´í„° ë¡œë”©...")
    train_df, test_df = load_data()

    if train_df is not None and test_df is not None:
        # === 2. íŠ¹ì§• ê³µí•™ (Feature Engineering) ===
        # ë¶„ì êµ¬ì¡°(SMILES)ë¡œë¶€í„° ëª¨ë¸ì´ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ìœ ì˜ë¯¸í•œ ìˆ«ì í˜•íƒœì˜ íŠ¹ì§•ë“¤ì„ ì¶”ì¶œí•˜ê³  ê°€ê³µí•˜ëŠ” ê³¼ì •
        print("\n2. íŠ¹ì§• ê³µí•™(Feature Engineering)...")
        
        # --- 2a. ChemBERTa ì„ë² ë”© ì¶”ì¶œ ---
        train_embeddings = get_chemberta_embeddings(train_df['Canonical_Smiles'].tolist(), CFG['CHEMBERTA_MODEL'])
        embedding_feature_names = [f"emb_{i}" for i in range(train_embeddings.shape[1])]
        embedding_df = pd.DataFrame(train_embeddings, columns=embedding_feature_names, index=train_df.index)

        # --- 2b. RDKit ë¶„ì ì„¤ëª…ì íŠ¹ì§• ì¶”ì¶œ ---
        train_df['descriptors'] = train_df['Canonical_Smiles'].apply(calculate_rdkit_descriptors)
        
        # ì„ë² ë”©ê³¼ ì„¤ëª…ì íŠ¹ì§• ê²°í•©
        train_df = pd.concat([train_df, embedding_df], axis=1)
        train_df.dropna(subset=['descriptors'], inplace=True) # ì„¤ëª…ì ê³„ì‚° ì‹¤íŒ¨í•œ ê²½ìš° ì œì™¸

        # íŠ¹ì§•ë“¤ì„ ìˆ˜í‰ìœ¼ë¡œ ê²°í•©í•˜ê¸° ìœ„í•´ NumPy ë°°ì—´ í˜•íƒœë¡œ ë³€í™˜
        desc_stack = np.stack(train_df['descriptors'].values)
        
        # ë¶„ì ì„¤ëª…ìì˜ ê²°ì¸¡ê°’(NaN) ì²˜ë¦¬
        # RDKitì´ íŠ¹ì • ë¶„ìì— ëŒ€í•´ ì„¤ëª…ìë¥¼ ê³„ì‚°í•˜ì§€ ëª»í•˜ëŠ” ê²½ìš° ë°œìƒ.
        # í›ˆë ¨ ë°ì´í„° ì „ì²´ì˜ ê° ì„¤ëª…ìë³„ í‰ê· ê°’ìœ¼ë¡œ ì´ ê²°ì¸¡ê°’ì„ ëŒ€ì²´.
        desc_mean = np.nanmean(desc_stack, axis=0)
        desc_stack = np.nan_to_num(desc_stack, nan=desc_mean)

        # ë¶„ì ì„¤ëª…ì ì •ê·œí™” (Standard Scaling)
        # ê° íŠ¹ì§•(ì—´)ì˜ í‰ê· ì„ 0, í‘œì¤€í¸ì°¨ë¥¼ 1ë¡œ ë§Œë“¤ì–´ì¤Œ.
        # ìŠ¤ì¼€ì¼ì´ ë‹¤ë¥¸ íŠ¹ì§•ë“¤ì´ ëª¨ë¸ í•™ìŠµì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ê· ë“±í•˜ê²Œ ë§Œë“¤ì–´ ì„±ëŠ¥ í–¥ìƒì— ë„ì›€.
        scaler = StandardScaler()
        desc_scaled = scaler.fit_transform(desc_stack)
        
        # === ìµœì¢… í›ˆë ¨ ë°ì´í„°ì…‹(X, y) ìƒì„± ===
        # ChemBERTa ì„ë² ë”©ê³¼ ì •ê·œí™”ëœ ë¶„ì ì„¤ëª…ìë¥¼ ìˆ˜í‰ìœ¼ë¡œ ê²°í•©í•˜ì—¬ ìµœì¢… íŠ¹ì§• í–‰ë ¬(X) ìƒì„±
        embedding_features = train_df[embedding_feature_names].values
        X = np.hstack([embedding_features, desc_scaled])
        # ì˜ˆì¸¡í•´ì•¼ í•  ëª©í‘œ ë³€ìˆ˜(y)ë¥¼ 'Inhibition' ì»¬ëŸ¼ìœ¼ë¡œ ì§€ì •
        y = train_df['Inhibition'].values

        # íŠ¹ì§• ì´ë¦„ ìƒì„± (LightGBM ì‹¤í–‰ ì‹œ ë°œìƒí•˜ëŠ” ê²½ê³  ë©”ì‹œì§€ë¥¼ ë°©ì§€í•˜ê³ , ë‚˜ì¤‘ì— íŠ¹ì§• ì¤‘ìš”ë„ ë¶„ì„ì„ ìš©ì´í•˜ê²Œ í•¨)
        desc_feature_names = [name for name, _ in Descriptors._descList]
        all_feature_names = embedding_feature_names + desc_feature_names
        # ìˆ«ìë§Œ ìˆë˜ NumPy ë°°ì—´ì„ íŠ¹ì§• ì´ë¦„ì´ ìˆëŠ” Pandas DataFrameìœ¼ë¡œ ë³€í™˜
        X = pd.DataFrame(X, columns=all_feature_names)

        # === 3. í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (Optuna) ===
        print("\n3. Optunaë¥¼ ì‚¬ìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”...")
        # Optuna ìŠ¤í„°ë”” ê°ì²´ ìƒì„± (ëª©í‘œ: objective í•¨ìˆ˜ì˜ ì ìˆ˜ë¥¼ 'ìµœëŒ€í™”(maximize)')
        study = optuna.create_study(direction='maximize', study_name='lgbm_inhibition_tuning')
        # ì •ì˜ëœ íšŸìˆ˜(N_TRIALS)ë§Œí¼ ìµœì í™” ìˆ˜í–‰
        study.optimize(lambda trial: objective(trial, X, y), n_trials=CFG['N_TRIALS'])

        print(f"\nìµœì í™” ì™„ë£Œ. ìµœê³  ì ìˆ˜: {study.best_value:.4f}")
        print("ìµœì  íŒŒë¼ë¯¸í„°:", study.best_params)

        # Optunaê°€ ì°¾ì€ ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ê¸°ë³¸ ëª¨ë¸ íŒŒë¼ë¯¸í„°ì— ì—…ë°ì´íŠ¸
        best_params = {
            'objective': 'regression', 
            'metric': 'rmse', 
            'verbose': -1, 
            'n_jobs': -1,
            'boosting_type': 'gbdt', 
            'n_estimators': 2000
        }
        best_params.update(study.best_params)

        # === 4. ìµœì¢… ëª¨ë¸ í›ˆë ¨ ë° ì˜ˆì¸¡ (ì‹œë“œ ì•™ìƒë¸”) ===
        print("\n4. ì‹œë“œ ì•™ìƒë¸”ì„ ì‚¬ìš©í•œ ìµœì¢… ëª¨ë¸ í›ˆë ¨ ë° ì˜ˆì¸¡...")
        
        # --- í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•´ì„œë„ í›ˆë ¨ ë°ì´í„°ì™€ 'ë™ì¼í•œ' íŠ¹ì§• ê³µí•™ ê³¼ì • ìˆ˜í–‰ ---
        # --- 4a. ChemBERTa ì„ë² ë”© ì¶”ì¶œ ---
        test_embeddings = get_chemberta_embeddings(test_df['Canonical_Smiles'].tolist(), CFG['CHEMBERTA_MODEL'])
        test_embedding_df = pd.DataFrame(test_embeddings, columns=embedding_feature_names, index=test_df.index)

        # --- 4b. RDKit ë¶„ì ì„¤ëª…ì íŠ¹ì§• ì¶”ì¶œ ---
        test_df['descriptors'] = test_df['Canonical_Smiles'].apply(calculate_rdkit_descriptors)
        
        # ì„ë² ë”©ê³¼ ì„¤ëª…ì íŠ¹ì§• ê²°í•©
        test_df = pd.concat([test_df, test_embedding_df], axis=1)

        # íŠ¹ì§• ì¶”ì¶œì— ì„±ê³µí•œ ìœ íš¨í•œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë§Œ ì„ íƒ (ì„ë² ë”©ì€ í•­ìƒ ì„±ê³µí•œë‹¤ê³  ê°€ì •)
        valid_test_mask = test_df['descriptors'].notna()
        
        # íŠ¹ì§•ë“¤ì„ NumPy ë°°ì—´ë¡œ ë³€í™˜
        embedding_test_features = test_df.loc[valid_test_mask, embedding_feature_names].values
        desc_test_stack = np.stack(test_df.loc[valid_test_mask, 'descriptors'].values)
        
        # ê²°ì¸¡ê°’ ì²˜ë¦¬ (ì¤‘ìš”: í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ í‰ê· ì´ ì•„ë‹Œ, 'í›ˆë ¨ ë°ì´í„°'ì—ì„œ ê³„ì‚°í•œ í‰ê· (desc_mean)ìœ¼ë¡œ ì±„ì›Œì•¼ í•¨)
        desc_test_stack = np.nan_to_num(desc_test_stack, nan=desc_mean)
        # ì •ê·œí™” (ì¤‘ìš”: í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ìƒˆë¡œ í•™ìŠµí•˜ëŠ” ê²ƒì´ ì•„ë‹Œ, 'í›ˆë ¨ ë°ì´í„°'ë¡œ í•™ìŠµëœ ìŠ¤ì¼€ì¼ëŸ¬(scaler)ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
        desc_test_scaled = scaler.transform(desc_test_stack)
        
        # ìµœì¢… í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹(X_test) ìƒì„±
        X_test = np.hstack([embedding_test_features, desc_test_scaled])
        X_test = pd.DataFrame(X_test, columns=all_feature_names)

        # ì‹œë“œ ì•™ìƒë¸”ì˜ ì „ì²´ ì˜ˆì¸¡ê°’ì„ ì €ì¥í•  ë°°ì—´ ì´ˆê¸°í™”
        ensembled_test_preds = np.zeros(len(X_test))
        
        # ì„¤ì •ëœ ì‹œë“œ ëª©ë¡ì„ í•˜ë‚˜ì”© ìˆœíšŒí•˜ë©° í›ˆë ¨ ë° ì˜ˆì¸¡ ë°˜ë³µ
        for seed in CFG['SEEDS']:
            print(f"--- í›ˆë ¨ ì‹œì‘ (ì‹œë“œ: {seed}) ---")
            seed_everything(seed)             # í˜„ì¬ ì‹œë“œë¡œ ëª¨ë“  ëœë¤ ìƒíƒœ ê³ ì •
            best_params['seed'] = seed        # ëª¨ë¸ íŒŒë¼ë¯¸í„°ì—ë„ í˜„ì¬ ì‹œë“œ ì„¤ì •
            
            # K-Fold ë¶„í• ê¸° (í˜„ì¬ ì‹œë“œë¡œ ì´ˆê¸°í™”í•˜ì—¬ ë§¤ë²ˆ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë¶„í• )
            kf = KFold(n_splits=CFG['N_SPLITS'], shuffle=True, random_state=seed)
            # í˜„ì¬ ì‹œë“œì—ì„œì˜ ì˜ˆì¸¡ê°’ì„ ì €ì¥í•  ë°°ì—´ ì´ˆê¸°í™” (í´ë“œë³„ ì˜ˆì¸¡ì„ í‰ê· ë‚´ê¸° ìœ„í•¨)
            seed_test_preds = np.zeros(len(X_test))

            # êµì°¨ ê²€ì¦ ë£¨í”„ (ì£¼ì˜: ì—¬ê¸°ì„œëŠ” ê²€ì¦ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³ , ê° í´ë“œë¥¼ ì „ì²´ ë°ì´í„°ë¡œ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨)
            # ì´ëŠ” ìµœì¢… ì˜ˆì¸¡ ì‹œ, ê°€ëŠ¥í•œ í•œ ë§ì€ ë°ì´í„°ë¡œ í›ˆë ¨ëœ ëª¨ë¸ì„ ë§Œë“¤ê¸° ìœ„í•¨. K-FoldëŠ” ë°ì´í„° ë¶„í•  ë°©ì‹ì˜ ë‹¤ì–‘ì„±ì„ ìœ„í•´ ì‚¬ìš©.
            for fold, (train_idx, _) in enumerate(kf.split(X, y)):
                print(f"--- í´ë“œ {fold+1}/{CFG['N_SPLITS']} (ì‹œë“œ: {seed}) ---")
                X_train, y_train = X.iloc[train_idx], y[train_idx]
                
                # ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ëª¨ë¸ ìƒì„±
                model = lgb.LGBMRegressor(**best_params)
                # í˜„ì¬ í´ë“œì˜ í›ˆë ¨ ë°ì´í„°ë¡œ ëª¨ë¸ í›ˆë ¨
                model.fit(X_train, y_train)
                # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ê³ , í´ë“œ ìˆ˜ë¡œ ë‚˜ëˆ„ì–´ ëˆ„ì  (í‰ê·  ê³„ì‚°)
                seed_test_preds += model.predict(X_test) / CFG['N_SPLITS']
            
            # í˜„ì¬ ì‹œë“œì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì „ì²´ ì•™ìƒë¸” ì˜ˆì¸¡ê°’ì— ë”í•¨
            ensembled_test_preds += seed_test_preds
            
        # ëª¨ë“  ì‹œë“œì˜ ì˜ˆì¸¡ê°’ì„ í•©í•œ ê²°ê³¼ë¥¼ ì‹œë“œì˜ ê°œìˆ˜ë¡œ ë‚˜ëˆ„ì–´ ìµœì¢… í‰ê·  ì˜ˆì¸¡ê°’ ê³„ì‚°
        final_preds = ensembled_test_preds / len(CFG['SEEDS'])
        # ì•ˆì •ì„±ì„ ìœ„í•´ ìµœì¢… ì˜ˆì¸¡ê°’ë„ 0~100 ì‚¬ì´ë¡œ í´ë¦¬í•‘
        final_preds = np.clip(final_preds, 0, 100)

        # === 5. ì œì¶œ íŒŒì¼ ìƒì„± ===
        print("\n5. ì œì¶œ íŒŒì¼ ìƒì„±...")
        data_dir = Path("./data")
        sample_submission = pd.read_csv(data_dir / "sample_submission.csv")
        
        # ì˜ˆì¸¡ ê²°ê³¼ë¥¼ 'ID'ì™€ 'Inhibition' ì»¬ëŸ¼ì„ ê°€ì§„ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
        pred_df = pd.DataFrame({'ID': test_df.loc[valid_test_mask, 'ID'], 'Inhibition': final_preds})
        
        # ëŒ€íšŒ ì œì¶œ ì–‘ì‹(sample_submission)ì— ë‚˜ì˜ ì˜ˆì¸¡ê°’ì„ ID ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©
        submission_df = sample_submission[['ID']].merge(pred_df, on='ID', how='left')
        # íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨ ë“±ìœ¼ë¡œ ì˜ˆì¸¡í•˜ì§€ ëª»í•œ ê°’ì´ ìˆë‹¤ë©´, í›ˆë ¨ ë°ì´í„°ì˜ ì „ì²´ í‰ê· ê°’ìœ¼ë¡œ ì±„ì›€
        submission_df['Inhibition'] = submission_df['Inhibition'].fillna(train_df['Inhibition'].mean())
        
        # ìµœì¢… ì œì¶œ íŒŒì¼ì„ 'submission.csv'ë¡œ ì €ì¥ (ì¸ë±ìŠ¤ëŠ” ì œì™¸)
        submission_path = Path("submission.csv")
        submission_df.to_csv(submission_path, index=False)
        print(f"ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {submission_path}")
        
        # --- ì˜ˆì¸¡ ê²°ê³¼ í†µê³„ ì¶œë ¥ ---
        print(f"\n--- ì˜ˆì¸¡ ê²°ê³¼ í†µê³„ ---")
        print(f"ì´ ì˜ˆì¸¡ ìˆ˜: {len(submission_df)}")
        print(f"ìœ íš¨í•œ ì˜ˆì¸¡ ìˆ˜: {len(pred_df)}")
        print(f"Inhibition ë²”ìœ„: {submission_df['Inhibition'].min():.2f}% ~ {submission_df['Inhibition'].max():.2f}%")
        print(f"í‰ê·  Inhibition: {submission_df['Inhibition'].mean():.2f}%")
        print(f"ì¤‘ì•™ê°’ Inhibition: {submission_df['Inhibition'].median():.2f}%")

        # === 6. ì˜ˆìƒ ìŠ¤ì½”ì–´ ê³„ì‚° ë° ì¶œë ¥ ===
        print("\n=== 6. ì˜ˆìƒ ìŠ¤ì½”ì–´ ê³„ì‚° (test.csvë¥¼ ì‹¤ì œ ë¦¬ë”ë³´ë“œ ë°ì´í„°ë¡œ ê°€ì •) ===")
        
        # test.csvì— ì‹¤ì œ ì •ë‹µê°’ì´ ìˆë‹¤ê³  ê°€ì •í•˜ê³  ìŠ¤ì½”ì–´ ê³„ì‚°
        # ì‹¤ì œë¡œëŠ” test.csvì— 'Inhibition' ì»¬ëŸ¼ì´ ì—†ìœ¼ë¯€ë¡œ, 
        # í›ˆë ¨ ë°ì´í„°ì˜ ë¶„í¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹œë®¬ë ˆì´ì…˜í•˜ê±°ë‚˜
        # ë˜ëŠ” ì‹¤ì œ ì •ë‹µê°’ì´ ìˆë‹¤ê³  ê°€ì •í•˜ê³  ê³„ì‚°
        
        try:
            # ë°©ë²• 1: test.csvì— ì‹¤ì œ ì •ë‹µê°’ì´ ìˆë‹¤ê³  ê°€ì •
            if 'Inhibition' in test_df.columns:
                # ì‹¤ì œ ì •ë‹µê°’ì´ ìˆëŠ” ê²½ìš°
                y_test_true = test_df.loc[valid_test_mask, 'Inhibition'].values
                expected_score = get_score(y_test_true, final_preds)
                print(f" ì˜ˆìƒ ìŠ¤ì½”ì–´ (ì‹¤ì œ ì •ë‹µ ê¸°ë°˜): {expected_score:.4f}")
                
                # ëª©í‘œ ì ìˆ˜ì™€ ë¹„êµ
                target_score = 0.85
                if expected_score >= target_score:
                    print(f"âœ… ëª©í‘œ ì ìˆ˜ {target_score} ë‹¬ì„±! (ì°¨ì´: +{expected_score - target_score:.4f})")
                else:
                    print(f"âŒ ëª©í‘œ ì ìˆ˜ {target_score} ë¯¸ë‹¬ì„± (ì°¨ì´: {expected_score - target_score:.4f})")
                    
            else:
                # ë°©ë²• 2: í›ˆë ¨ ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ì˜ˆìƒ ìŠ¤ì½”ì–´ ì‹œë®¬ë ˆì´ì…˜
                print("ğŸ“Š test.csvì— ì •ë‹µê°’ì´ ì—†ìœ¼ë¯€ë¡œ í›ˆë ¨ ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ì˜ˆìƒ ìŠ¤ì½”ì–´ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤...")
                
                # í›ˆë ¨ ë°ì´í„°ì—ì„œ êµì°¨ ê²€ì¦ ìŠ¤ì½”ì–´ ê³„ì‚°
                print("\n--- êµì°¨ ê²€ì¦ ìŠ¤ì½”ì–´ ê³„ì‚° ---")
                cv_scores = []
                
                # ì‹œë“œ ì•™ìƒë¸”ì˜ ê° ì‹œë“œë³„ë¡œ êµì°¨ ê²€ì¦ ìŠ¤ì½”ì–´ ê³„ì‚°
                for seed in CFG['SEEDS']:
                    print(f"\n--- ì‹œë“œ {seed} êµì°¨ ê²€ì¦ ---")
                    seed_everything(seed)
                    best_params['seed'] = seed
                    
                    kf = KFold(n_splits=CFG['N_SPLITS'], shuffle=True, random_state=seed)
                    oof_preds = np.zeros(len(X))
                    
                    for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):
                        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
                        y_train, y_val = y[train_idx], y[val_idx]
                        
                        model = lgb.LGBMRegressor(**best_params)
                        model.fit(X_train, y_train, 
                                eval_set=[(X_val, y_val)],
                                eval_metric=lgbm_score_metric,
                                callbacks=[lgb.early_stopping(100, verbose=False)])
                        
                        oof_preds[val_idx] = np.clip(model.predict(X_val), 0, 100)
                    
                    # í˜„ì¬ ì‹œë“œì˜ êµì°¨ ê²€ì¦ ìŠ¤ì½”ì–´ ê³„ì‚°
                    cv_score = get_score(y, oof_preds)
                    cv_scores.append(cv_score)
                    print(f"ì‹œë“œ {seed} CV ìŠ¤ì½”ì–´: {cv_score:.4f}")
                
                # ì „ì²´ ì‹œë“œì˜ í‰ê·  êµì°¨ ê²€ì¦ ìŠ¤ì½”ì–´
                mean_cv_score = np.mean(cv_scores)
                std_cv_score = np.std(cv_scores)
                
                print(f"\nğŸ“ˆ êµì°¨ ê²€ì¦ ê²°ê³¼:")
                print(f"í‰ê·  CV ìŠ¤ì½”ì–´: {mean_cv_score:.4f} Â± {std_cv_score:.4f}")
                print(f"ìµœê³  CV ìŠ¤ì½”ì–´: {np.max(cv_scores):.4f}")
                print(f"ìµœì € CV ìŠ¤ì½”ì–´: {np.min(cv_scores):.4f}")
                
                # ëª©í‘œ ì ìˆ˜ì™€ ë¹„êµ
                target_score = 0.85
                if mean_cv_score >= target_score:
                    print(f"âœ… ëª©í‘œ ì ìˆ˜ {target_score} ë‹¬ì„± ê°€ëŠ¥ì„± ë†’ìŒ!")
                    print(f"   (í‰ê·  CV ìŠ¤ì½”ì–´: {mean_cv_score:.4f})")
                else:
                    print(f"âš ï¸  ëª©í‘œ ì ìˆ˜ {target_score} ë‹¬ì„±ì— ë„ì „ì ")
                    print(f"   (í‰ê·  CV ìŠ¤ì½”ì–´: {mean_cv_score:.4f}, ì°¨ì´: {mean_cv_score - target_score:.4f})")
                
                # ì„±ëŠ¥ ê°œì„  ì œì•ˆ
                print(f"\nğŸ’¡ ì„±ëŠ¥ ê°œì„  ì œì•ˆ:")
                if mean_cv_score < 0.80:
                    print("   - ë” ë§ì€ ì‚¬ì „ í›ˆë ¨ ëª¨ë¸ ì•™ìƒë¸” ê³ ë ¤")
                    print("   - ì¶”ê°€ì ì¸ ë¶„ì ì„¤ëª…ì í™œìš©")
                    print("   - í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ë²”ìœ„ í™•ëŒ€")
                elif mean_cv_score < 0.85:
                    print("   - ì‹œë“œ ì•™ìƒë¸” ìˆ˜ ì¦ê°€")
                    print("   - ë” ì •êµí•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹")
                    print("   - íŠ¹ì§• ì„ íƒ ê¸°ë²• ì ìš©")
                else:
                    print("   - í˜„ì¬ ëª¨ë¸ì´ ëª©í‘œ ì„±ëŠ¥ì„ ì¶©ì¡±í•©ë‹ˆë‹¤!")
                
                # ì˜ˆìƒ ë¦¬ë”ë³´ë“œ ìŠ¤ì½”ì–´ (êµì°¨ ê²€ì¦ ìŠ¤ì½”ì–´ì— ì•½ê°„ì˜ ë³´ìˆ˜ì  ì¡°ì •)
                expected_leaderboard_score = mean_cv_score - 0.02  # ë³´ìˆ˜ì  ì¶”ì •
                print(f"\nğŸ¯ ì˜ˆìƒ ë¦¬ë”ë³´ë“œ ìŠ¤ì½”ì–´: {expected_leaderboard_score:.4f}")
                print(f"   (CV ìŠ¤ì½”ì–´ì—ì„œ 0.02ë¥¼ ëº€ ë³´ìˆ˜ì  ì¶”ì •)")
                
        except Exception as e:
            print(f"ìŠ¤ì½”ì–´ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print("ê¸°ë³¸ í†µê³„ ì •ë³´ë§Œ ì¶œë ¥í•©ë‹ˆë‹¤.")
        
        print(f"\n{'='*60}")
        print("ğŸ‰ ëª¨ë¸ í›ˆë ¨ ë° ì˜ˆì¸¡ ì™„ë£Œ!")
        print(f"{'='*60}")
        
    else:
        print("ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨. íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
