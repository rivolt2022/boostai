# ğŸ”¥ ë‹¤ì¤‘ ì•Œê³ ë¦¬ì¦˜ CYP3A4 ì˜ˆì¸¡ ëª¨ë¸ (0.70+ ëŒíŒŒ!)
import pandas as pd
import numpy as np
import os
import random
import pickle
import warnings
from rdkit import Chem
from rdkit.Chem import AllChem, DataStructs, Descriptors
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.model_selection import KFold, StratifiedKFold
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.neural_network import MLPRegressor
from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor
import optuna

# ì•Œê³ ë¦¬ì¦˜ë³„ ì„í¬íŠ¸
import lightgbm as lgb
try:
    import xgboost as xgb
    HAS_XGB = True
except ImportError:
    HAS_XGB = False
    
try:
    import catboost as cb
    HAS_CATBOOST = True
except ImportError:
    HAS_CATBOOST = False

warnings.filterwarnings('ignore')

# ğŸ”¥ ë‹¤ì¤‘ ì•Œê³ ë¦¬ì¦˜ ì„¤ì •
CFG = {
    'SEEDS': [42, 123, 456, 789, 999, 2023, 2024],  # 7ê°œ ì‹œë“œë¡œ í™•ëŒ€
    'N_SPLITS': 10,     # 10-fold CV
    'N_TRIALS': 75,     # ê° ì•Œê³ ë¦¬ì¦˜ë³„ ìµœì í™” ì‹œë„
    'ALGORITHMS': ['xgb', 'catboost', 'neural', 'lgb', 'rf'],  # 5ê°œ ì•Œê³ ë¦¬ì¦˜
    'ENSEMBLE_METHOD': 'weighted',  # weighted, stacking
}

def seed_everything(seed):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)

seed_everything(CFG['SEEDS'][0])

class MultiAlgorithmPredictor:
    def __init__(self):
        self.algorithms = {}
        self.best_params = {}
        self.scaler = StandardScaler()
        self.cv_scores = {}
        self.algorithm_weights = {}
        
    def extract_features(self, smiles):
        """ğŸ”¬ í•µì‹¬ íŠ¹ì„± ì¶”ì¶œ (ì†ë„ ìµœì í™”)"""
        mol = Chem.MolFromSmiles(smiles)
        if mol is None:
            return None
        
        # 1. Morgan Fingerprint (radius=2, 1024 bits)
        fp_morgan = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=1024)
        arr_morgan = np.zeros((1024,))
        DataStructs.ConvertToNumpyArray(fp_morgan, arr_morgan)
        
        # 2. MACCS Keys
        fp_maccs = AllChem.GetMACCSKeysFingerprint(mol)
        arr_maccs = np.zeros((167,))
        DataStructs.ConvertToNumpyArray(fp_maccs, arr_maccs)
        
        # 3. í•µì‹¬ RDKit Descriptors (ì†ë„ ì¤‘ì‹œ)
        core_descriptors = []
        important_descs = [
            'MolWt', 'MolLogP', 'NumHDonors', 'NumHAcceptors', 'TPSA',
            'NumRotatableBonds', 'NumAromaticRings', 'NumHeteroatoms'
        ]
        
        for desc_name in important_descs:
            try:
                desc_func = getattr(Descriptors, desc_name)
                desc_value = desc_func(mol)
                core_descriptors.append(desc_value if desc_value is not None else 0)
            except:
                core_descriptors.append(0)
        
        # íŠ¹ì„± ê²°í•©
        features = np.concatenate([arr_morgan, arr_maccs, core_descriptors])
        return features
    
    def prepare_data(self, df, is_training=True):
        """ğŸ“Š ë°ì´í„° ì „ì²˜ë¦¬"""
        smiles_col = 'Canonical_Smiles' if 'Canonical_Smiles' in df.columns else 'SMILES'
        
        print(f"íŠ¹ì„± ì¶”ì¶œ ì¤‘... ({len(df)}ê°œ ë¶„ì)")
        features_list = []
        valid_indices = []
        
        for idx, smiles in enumerate(df[smiles_col]):
            features = self.extract_features(smiles)
            if features is not None:
                features_list.append(features)
                valid_indices.append(idx)
            
            if (idx + 1) % 100 == 0:
                print(f"  ì§„í–‰: {idx + 1}/{len(df)}")
        
        X = np.array(features_list)
        X = np.nan_to_num(X, nan=0, posinf=0, neginf=0)
        
        # ìŠ¤ì¼€ì¼ë§
        if is_training:
            X = self.scaler.fit_transform(X)
        else:
            X = self.scaler.transform(X)
        
        print(f"âœ… íŠ¹ì„± ì¶”ì¶œ ì™„ë£Œ: {X.shape[1]:,}ê°œ íŠ¹ì„±")
        return X, valid_indices

    def get_score(self, y_true, y_pred):
        """ë¦¬ë”ë³´ë“œ ìŠ¤ì½”ì–´ ê³„ì‚°"""
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        y_range = y_true.max() - y_true.min()
        nrmse = rmse / y_range if y_range > 0 else 0
        
        correlation = np.corrcoef(y_true, y_pred)[0, 1]
        if np.isnan(correlation):
            correlation = 0
        
        score = 0.5 * (1 - nrmse) + 0.5 * correlation
        return score

    def optimize_xgboost(self, X, y):
        """ğŸš€ XGBoost ìµœì í™”"""
        if not HAS_XGB:
            print("âŒ XGBoostë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
            
        print("ğŸš€ XGBoost í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”...")
        
        def objective(trial):
            params = {
                'objective': 'reg:squarederror',
                'n_estimators': trial.suggest_int('n_estimators', 500, 2000),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.15),
                'max_depth': trial.suggest_int('max_depth', 3, 12),
                'subsample': trial.suggest_float('subsample', 0.6, 0.95),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.95),
                'reg_alpha': trial.suggest_float('reg_alpha', 0.001, 1.0),
                'reg_lambda': trial.suggest_float('reg_lambda', 0.001, 1.0),
                'random_state': CFG['SEEDS'][0],
                'n_jobs': -1,
                'verbosity': 0,
            }
            
            kf = KFold(n_splits=CFG['N_SPLITS'], shuffle=True, random_state=CFG['SEEDS'][0])
            scores = []
            
            for train_idx, val_idx in kf.split(X, y):
                X_train, X_val = X[train_idx], X[val_idx]
                y_train, y_val = y[train_idx], y[val_idx]
                
                model = xgb.XGBRegressor(**params)
                model.fit(X_train, y_train)
                
                y_pred = model.predict(X_val)
                score = self.get_score(y_val, y_pred)
                scores.append(score)
            
            return np.mean(scores)
        
        study = optuna.create_study(direction='maximize', study_name='xgb_opt')
        study.optimize(objective, n_trials=CFG['N_TRIALS'])
        
        print(f"âœ… XGBoost ìµœì í™” ì™„ë£Œ! ìŠ¤ì½”ì–´: {study.best_value:.4f}")
        return study.best_params, study.best_value

    def optimize_catboost(self, X, y):
        """ğŸ± CatBoost ìµœì í™”"""
        if not HAS_CATBOOST:
            print("âŒ CatBoostë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
            
        print("ğŸ± CatBoost í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”...")
        
        def objective(trial):
            params = {
                'iterations': trial.suggest_int('iterations', 500, 2000),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.15),
                'depth': trial.suggest_int('depth', 4, 10),
                'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),
                'bootstrap_type': trial.suggest_categorical('bootstrap_type', ['Bayesian', 'Bernoulli']),
                'random_seed': CFG['SEEDS'][0],
                'thread_count': -1,
                'verbose': False,
            }
            
            if params['bootstrap_type'] == 'Bayesian':
                params['bagging_temperature'] = trial.suggest_float('bagging_temperature', 0, 1)
            else:
                params['subsample'] = trial.suggest_float('subsample', 0.6, 0.95)
            
            kf = KFold(n_splits=CFG['N_SPLITS'], shuffle=True, random_state=CFG['SEEDS'][0])
            scores = []
            
            for train_idx, val_idx in kf.split(X, y):
                X_train, X_val = X[train_idx], X[val_idx]
                y_train, y_val = y[train_idx], y[val_idx]
                
                model = cb.CatBoostRegressor(**params)
                model.fit(X_train, y_train)
                
                y_pred = model.predict(X_val)
                score = self.get_score(y_val, y_pred)
                scores.append(score)
            
            return np.mean(scores)
        
        study = optuna.create_study(direction='maximize', study_name='cat_opt')
        study.optimize(objective, n_trials=CFG['N_TRIALS'])
        
        print(f"âœ… CatBoost ìµœì í™” ì™„ë£Œ! ìŠ¤ì½”ì–´: {study.best_value:.4f}")
        return study.best_params, study.best_value

    def optimize_neural_network(self, X, y):
        """ğŸ§  Neural Network ìµœì í™”"""
        print("ğŸ§  Neural Network í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”...")
        
        def objective(trial):
            # íˆë“  ë ˆì´ì–´ êµ¬ì¡° ìµœì í™”
            n_layers = trial.suggest_int('n_layers', 2, 4)
            hidden_sizes = []
            
            for i in range(n_layers):
                size = trial.suggest_int(f'hidden_size_{i}', 50, 500)
                hidden_sizes.append(size)
            
            params = {
                'hidden_layer_sizes': tuple(hidden_sizes),
                'activation': trial.suggest_categorical('activation', ['relu', 'tanh']),
                'solver': trial.suggest_categorical('solver', ['adam', 'lbfgs']),
                'alpha': trial.suggest_float('alpha', 0.0001, 0.1, log=True),
                'learning_rate': trial.suggest_categorical('learning_rate', ['constant', 'adaptive']),
                'max_iter': trial.suggest_int('max_iter', 500, 2000),
                'early_stopping': True,
                'validation_fraction': 0.15,
                'n_iter_no_change': 50,
                'random_state': CFG['SEEDS'][0],
            }
            
            if params['solver'] == 'adam':
                params['learning_rate_init'] = trial.suggest_float('learning_rate_init', 0.0001, 0.01, log=True)
            
            kf = KFold(n_splits=CFG['N_SPLITS'], shuffle=True, random_state=CFG['SEEDS'][0])
            scores = []
            
            for train_idx, val_idx in kf.split(X, y):
                X_train, X_val = X[train_idx], X[val_idx]
                y_train, y_val = y[train_idx], y[val_idx]
                
                try:
                    model = MLPRegressor(**params)
                    model.fit(X_train, y_train)
                    
                    y_pred = model.predict(X_val)
                    score = self.get_score(y_val, y_pred)
                    scores.append(score)
                except:
                    scores.append(0.0)  # ì‹¤íŒ¨ì‹œ ë‚®ì€ ì ìˆ˜
            
            return np.mean(scores)
        
        study = optuna.create_study(direction='maximize', study_name='nn_opt')
        study.optimize(objective, n_trials=CFG['N_TRIALS'])
        
        print(f"âœ… Neural Network ìµœì í™” ì™„ë£Œ! ìŠ¤ì½”ì–´: {study.best_value:.4f}")
        return study.best_params, study.best_value

    def optimize_lightgbm(self, X, y):
        """ğŸ’¡ LightGBM ìµœì í™” (ì°¸ê³ ìš©)"""
        print("ğŸ’¡ LightGBM í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”...")
        
        def objective(trial):
            params = {
                'objective': 'regression',
                'metric': 'rmse',
                'verbose': -1,
                'n_jobs': -1,
                'seed': CFG['SEEDS'][0],
                'boosting_type': 'gbdt',
                'n_estimators': trial.suggest_int('n_estimators', 500, 2000),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.15),
                'num_leaves': trial.suggest_int('num_leaves', 15, 100),
                'max_depth': trial.suggest_int('max_depth', 3, 12),
                'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 0.95),
                'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 0.95),
                'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),
                'min_child_samples': trial.suggest_int('min_child_samples', 10, 50),
                'lambda_l1': trial.suggest_float('lambda_l1', 0.001, 1.0),
                'lambda_l2': trial.suggest_float('lambda_l2', 0.001, 1.0),
            }
            
            kf = KFold(n_splits=CFG['N_SPLITS'], shuffle=True, random_state=CFG['SEEDS'][0])
            scores = []
            
            for train_idx, val_idx in kf.split(X, y):
                X_train, X_val = X[train_idx], X[val_idx]
                y_train, y_val = y[train_idx], y[val_idx]
                
                model = lgb.LGBMRegressor(**params)
                model.fit(X_train, y_train)
                
                y_pred = model.predict(X_val)
                score = self.get_score(y_val, y_pred)
                scores.append(score)
            
            return np.mean(scores)
        
        study = optuna.create_study(direction='maximize', study_name='lgb_opt')
        study.optimize(objective, n_trials=CFG['N_TRIALS'])
        
        print(f"âœ… LightGBM ìµœì í™” ì™„ë£Œ! ìŠ¤ì½”ì–´: {study.best_value:.4f}")
        return study.best_params, study.best_value

    def optimize_random_forest(self, X, y):
        """ğŸŒ² Random Forest ìµœì í™”"""
        print("ğŸŒ² Random Forest í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”...")
        
        def objective(trial):
            params = {
                'n_estimators': trial.suggest_int('n_estimators', 500, 1500),
                'max_depth': trial.suggest_int('max_depth', 5, 20),
                'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),
                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 5),
                'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),
                'bootstrap': trial.suggest_categorical('bootstrap', [True, False]),
                'random_state': CFG['SEEDS'][0],
                'n_jobs': -1,
            }
            
            kf = KFold(n_splits=CFG['N_SPLITS'], shuffle=True, random_state=CFG['SEEDS'][0])
            scores = []
            
            for train_idx, val_idx in kf.split(X, y):
                X_train, X_val = X[train_idx], X[val_idx]
                y_train, y_val = y[train_idx], y[val_idx]
                
                model = RandomForestRegressor(**params)
                model.fit(X_train, y_train)
                
                y_pred = model.predict(X_val)
                score = self.get_score(y_val, y_pred)
                scores.append(score)
            
            return np.mean(scores)
        
        study = optuna.create_study(direction='maximize', study_name='rf_opt')
        study.optimize(objective, n_trials=CFG['N_TRIALS'])
        
        print(f"âœ… Random Forest ìµœì í™” ì™„ë£Œ! ìŠ¤ì½”ì–´: {study.best_value:.4f}")
        return study.best_params, study.best_value

    def train_all_algorithms(self, X_train, y_train):
        """ğŸ”¥ ëª¨ë“  ì•Œê³ ë¦¬ì¦˜ í›ˆë ¨ ë° ìµœì í™”"""
        print("ğŸ”¥ ë‹¤ì¤‘ ì•Œê³ ë¦¬ì¦˜ ìµœì í™” ì‹œì‘!")
        print(f"ğŸ“Š ì•Œê³ ë¦¬ì¦˜: {CFG['ALGORITHMS']}")
        print(f"ğŸ¯ ê° ì•Œê³ ë¦¬ì¦˜ë³„ {CFG['N_TRIALS']}íšŒ ìµœì í™”")
        
        results = {}
        
        # ê° ì•Œê³ ë¦¬ì¦˜ë³„ ìµœì í™”
        for algorithm in CFG['ALGORITHMS']:
            print(f"\n{'='*50}")
            print(f"ğŸš€ {algorithm.upper()} ìµœì í™” ì‹œì‘...")
            
            try:
                if algorithm == 'xgb' and HAS_XGB:
                    params, score = self.optimize_xgboost(X_train, y_train)
                elif algorithm == 'catboost' and HAS_CATBOOST:
                    params, score = self.optimize_catboost(X_train, y_train)
                elif algorithm == 'neural':
                    params, score = self.optimize_neural_network(X_train, y_train)
                elif algorithm == 'lgb':
                    params, score = self.optimize_lightgbm(X_train, y_train)
                elif algorithm == 'rf':
                    params, score = self.optimize_random_forest(X_train, y_train)
                else:
                    print(f"âŒ {algorithm} ì‚¬ìš© ë¶ˆê°€ëŠ¥")
                    continue
                    
                if params is not None:
                    results[algorithm] = {
                        'params': params,
                        'score': score
                    }
                    print(f"âœ… {algorithm} ì™„ë£Œ: {score:.4f}")
                    
            except Exception as e:
                print(f"âŒ {algorithm} ì˜¤ë¥˜: {e}")
                continue
        
        # ê²°ê³¼ ì •ë¦¬
        if results:
            print(f"\nğŸ† ì•Œê³ ë¦¬ì¦˜ë³„ ì„±ëŠ¥ ìˆœìœ„:")
            sorted_results = sorted(results.items(), key=lambda x: x[1]['score'], reverse=True)
            
            for i, (alg, result) in enumerate(sorted_results, 1):
                print(f"  {i}. {alg.upper()}: {result['score']:.4f}")
                
            # ê°€ì¤‘ì¹˜ ê³„ì‚° (ì„±ëŠ¥ ê¸°ë°˜)
            total_score = sum(result['score'] for result in results.values())
            self.algorithm_weights = {
                alg: result['score'] / total_score 
                for alg, result in results.items()
            }
            
            print(f"\nâš–ï¸ ì„±ëŠ¥ ê¸°ë°˜ ê°€ì¤‘ì¹˜:")
            for alg, weight in self.algorithm_weights.items():
                print(f"  {alg.upper()}: {weight:.3f}")
        
        self.best_params = results
        return results

    def predict_ensemble(self, X_test):
        """ğŸ¯ ë‹¤ì¤‘ ì•Œê³ ë¦¬ì¦˜ ì•™ìƒë¸” ì˜ˆì¸¡"""
        print("ğŸ¯ ë‹¤ì¤‘ ì•Œê³ ë¦¬ì¦˜ ì•™ìƒë¸” ì˜ˆì¸¡...")
        
        all_predictions = {}
        X_train = self.X_train_stored
        y_train = self.y_train_stored
        
        # ê° ì•Œê³ ë¦¬ì¦˜ë³„ ì˜ˆì¸¡
        for algorithm in self.best_params.keys():
            print(f"ğŸ”„ {algorithm.upper()} ì•™ìƒë¸” ì˜ˆì¸¡...")
            
            algorithm_preds = []
            
            # ë‹¤ì¤‘ ì‹œë“œ ì•™ìƒë¸”
            for seed in CFG['SEEDS']:
                kf = KFold(n_splits=CFG['N_SPLITS'], shuffle=True, random_state=seed)
                seed_preds = np.zeros(len(X_test))
                
                params = self.best_params[algorithm]['params'].copy()
                
                for fold, (train_idx, _) in enumerate(kf.split(X_train, y_train)):
                    X_fold_train = X_train[train_idx]
                    y_fold_train = y_train[train_idx]
                    
                    # ì•Œê³ ë¦¬ì¦˜ë³„ ëª¨ë¸ ìƒì„±
                    if algorithm == 'xgb':
                        params['random_state'] = seed
                        model = xgb.XGBRegressor(**params)
                    elif algorithm == 'catboost':
                        params['random_seed'] = seed
                        model = cb.CatBoostRegressor(**params)
                    elif algorithm == 'neural':
                        params['random_state'] = seed
                        model = MLPRegressor(**params)
                    elif algorithm == 'lgb':
                        params['seed'] = seed
                        model = lgb.LGBMRegressor(**params)
                    elif algorithm == 'rf':
                        params['random_state'] = seed
                        model = RandomForestRegressor(**params)
                    
                    model.fit(X_fold_train, y_fold_train)
                    fold_pred = model.predict(X_test)
                    seed_preds += fold_pred / CFG['N_SPLITS']
                
                algorithm_preds.append(seed_preds)
            
            # ì‹œë“œë³„ í‰ê· 
            all_predictions[algorithm] = np.mean(algorithm_preds, axis=0)
        
        # ê°€ì¤‘ í‰ê·  ì•™ìƒë¸”
        final_predictions = np.zeros(len(X_test))
        for algorithm, predictions in all_predictions.items():
            weight = self.algorithm_weights[algorithm]
            final_predictions += weight * predictions
        
        # í›„ì²˜ë¦¬
        final_predictions = np.clip(final_predictions, 0, 100)
        
        print(f"âœ… ë‹¤ì¤‘ ì•Œê³ ë¦¬ì¦˜ ì•™ìƒë¸” ì™„ë£Œ!")
        return final_predictions

    def store_training_data(self, X_train, y_train):
        self.X_train_stored = X_train
        self.y_train_stored = y_train

def main():
    print("ğŸ”¥ ë‹¤ì¤‘ ì•Œê³ ë¦¬ì¦˜ CYP3A4 ì˜ˆì¸¡ ëª¨ë¸ ğŸ”¥")
    print("=" * 70)
    print("ğŸš€ ì „ëµ: XGBoost + CatBoost + Neural Network + LightGBM + RandomForest")
    print("ğŸ¯ ëª©í‘œ: 0.70+ ëŒíŒŒ (ì™„ì „íˆ ë‹¤ë¥¸ ì ‘ê·¼ë²•)")
    print("âš¡ ê° ì•Œê³ ë¦¬ì¦˜ ë…ë¦½ ìµœì í™” â†’ ì„±ëŠ¥ ê¸°ë°˜ ê°€ì¤‘ ì•™ìƒë¸”")
    
    try:
        # ë°ì´í„° ë¡œë“œ
        print("\nğŸ“ ë°ì´í„° ë¡œë“œ...")
        train_df = pd.read_csv('data/train.csv')
        test_df = pd.read_csv('data/test.csv')
        sample_submission = pd.read_csv('data/sample_submission.csv')
        
        print(f"í›ˆë ¨: {len(train_df)}ê°œ, í…ŒìŠ¤íŠ¸: {len(test_df)}ê°œ")
        print(f"Inhibition ë²”ìœ„: {train_df['Inhibition'].min():.1f} ~ {train_df['Inhibition'].max():.1f}")
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        predictor = MultiAlgorithmPredictor()
        
        # íŠ¹ì„± ì¶”ì¶œ
        X_train, train_valid_idx = predictor.prepare_data(train_df, is_training=True)
        X_test, test_valid_idx = predictor.prepare_data(test_df, is_training=False)
        
        y_train = train_df.iloc[train_valid_idx]['Inhibition'].values
        
        print(f"âœ… ìµœì¢… ë°ì´í„°: í›ˆë ¨ {X_train.shape}, í…ŒìŠ¤íŠ¸ {X_test.shape}")
        
        # í›ˆë ¨ ë°ì´í„° ì €ì¥
        predictor.store_training_data(X_train, y_train)
        
        # ëª¨ë“  ì•Œê³ ë¦¬ì¦˜ í›ˆë ¨
        print("\nğŸ”¥ ë‹¤ì¤‘ ì•Œê³ ë¦¬ì¦˜ í›ˆë ¨ ì‹œì‘...")
        results = predictor.train_all_algorithms(X_train, y_train)
        
        if not results:
            print("âŒ ëª¨ë“  ì•Œê³ ë¦¬ì¦˜ ì‹¤íŒ¨!")
            return
        
        # ì•™ìƒë¸” ì˜ˆì¸¡
        print("\nğŸ¯ ë‹¤ì¤‘ ì•Œê³ ë¦¬ì¦˜ ì•™ìƒë¸” ì˜ˆì¸¡...")
        predictions = predictor.predict_ensemble(X_test)
        
        # ì œì¶œ íŒŒì¼ ìƒì„±
        submission = sample_submission.copy()
        submission.iloc[test_valid_idx, submission.columns.get_loc('Inhibition')] = predictions
        
        # ì˜ˆì¸¡í•˜ì§€ ëª»í•œ ë¶€ë¶„ì€ í‰ê· ê°’ìœ¼ë¡œ ì±„ì›€
        mean_inhibition = train_df['Inhibition'].mean()
        submission['Inhibition'].fillna(mean_inhibition, inplace=True)
        
        # ì €ì¥
        output_file = 'submission_multi_algorithm.csv'
        submission.to_csv(output_file, index=False)
        
        print(f"\nâœ… ë‹¤ì¤‘ ì•Œê³ ë¦¬ì¦˜ ì œì¶œ íŒŒì¼ ì €ì¥: {output_file}")
        print(f"ğŸ”¥ ìµœê³  ì„±ëŠ¥ ì•Œê³ ë¦¬ì¦˜: {max(results.items(), key=lambda x: x[1]['score'])}")
        print(f"\nğŸ“Š ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼:")
        print(submission['Inhibition'].describe())
        
        print(f"\nğŸ”¥ ë‹¤ì¤‘ ì•Œê³ ë¦¬ì¦˜ ëª¨ë¸ ì™„ë£Œ!")
        print(f"ğŸš€ XGBoost + CatBoost + Neural Network + LightGBM + RandomForest")
        print(f"âš–ï¸ ì„±ëŠ¥ ê¸°ë°˜ ê°€ì¤‘ ì•™ìƒë¸”")
        print(f"ğŸ¯ ê¸°ëŒ€ íš¨ê³¼: 0.70+ ëŒíŒŒ!")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 